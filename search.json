[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Catal√†English\n\n\nAqu√≠ trobar√†s contingut molt variat; desde sistemes de posicionament global (GNSS), desenvolupament de software, algoritmes i jocs de taula. Pot ser que sigui interessants o avorrides, dep√®n de tu.\nA m√©s, per complicar-ho una mica, hi ha una barreja de lleng√ºes bastant considerable (angl√®s, catal√†, espanyol i gallec), per√≤ suposo que sempre pots fer servir Google Translate si t‚Äôinteressa algun contingut que estigui en un idioma que no domines üòâ.\n\n\nHere you will find a medley of content; from navigation systems (GNSS), software development, algorithms and board games. They can be both interesting or boring, at the end this is up to you.\nIn addition, to make things a bit more complicated, there is a considerable mix of languages (English, Catalan, Spanish, and Galician), but I suppose you can always use Google Translate if you‚Äôre interested in any content that‚Äôs in a language you‚Äôre not fluent in üòâ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Apache parquet to store and process GNSS measurements\n\n\n\n\n\n\nGNSS\n\n\nEnglish\n\n\n\nLeverage Apache parquet to store and exchange GNSS measurement, usually stored in RINEX (text-based) format. This format can be quickly loaded in data structures such as pandas DataFrame for efficient data manipulation and processing.\n\n\n\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOn the exploding size of GNSS measurement data files\n\n\n\n\n\n\nGNSS\n\n\nEnglish\n\n\n\nThe rapid growth of GNSS receiver networks, coupled with the proliferation of constellations and signals, has led to an exponential increase in data volume. This post uses the GNSS data repository of the CDDIS GNSS receiver network to illustrate this trend.\n\n\n\n\n\nOct 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTraduci√≥n do xogo ‚ÄúC√≥digo Secreto‚Äù √≥ galego\n\n\n\n\n\n\nJocs de taula\n\n\nGalego\n\n\n\nIdeas para a traduci√≥n das cartas de palabras do xogo ‚ÄúCodenames‚Äù\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCom fer el teu Cronocartes\n\n\n\n\n\n\nJocs de taula\n\n\nCatal√†\n\n\n\nApunts per crear la teva versi√≥ del joc de cartes hist√≤ric Cronocartes\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGraph theory to model Ticket to ride\n\n\n\n\n\n\ngames\n\n\nEnglish\n\n\n\nHow to use Graph theory to build a model for the Ticket to ride boardgame as a first step towards customization and create your own maps.\n\n\n\n\n\nDec 24, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html",
    "title": "Com fer el teu Cronocartes",
    "section": "",
    "text": "El joc Cronocartes Hist√≤ria de Catalunya √©s un joc de cartes r√†pid i familiar en qu√® els jugadors competeixen ordenant una l√≠nia cronol√≤gica. Est√† basat en els populars jocs de cartes Timeline o Cardline.\nEl joc √©s f√†cilment portable a altres temes que requereixin una cronologia (moments √®pics de la saga Star Wars, hist√≤ria del barri de Sants de Barcelona‚Ä¶). De fet, diverses persones interessades en el format m‚Äôhan preguntat per consells de com fer una versi√≥ del Cronocartes, aix√≠ que us deixo aquest post amb alguns apunts i consells sobre el disseny per si us poden ser √∫tils."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#comen√ßa-la-casa-pel-terrat",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#comen√ßa-la-casa-pel-terrat",
    "title": "Com fer el teu Cronocartes",
    "section": "Comen√ßa la casa pel terrat",
    "text": "Comen√ßa la casa pel terrat\nNo s√≥c dissenyador gr√†fic, i una de les pors m√©s grans que tenia era fer un disseny que despr√©s hagu√©s d‚Äôanar modificant en funci√≥ del servei d‚Äôimpremta. Aix√≠ que, un cop decidit a fer la versi√≥ catalana del Cronocartes, vaig buscar un servei d‚Äôimpremta que tingu√©s aquestes caracter√≠stiques:\n\nPoder tenir un pressupost immediat via web, sense haver d‚Äôenviar formularis web, i fer-me una idea r√†pida de quan pujaria el cost m√©s important de la producci√≥.\nQue oferissin la mateixa mida de cartes que el Timeline per tal de poder-les combinar (Mini USA 41 x 63 mm)\nDisponibilitat de desc√†rrega de la plantilla per fer les cartes.\nFormat de la plantilla suportat per algun programari lliure (mireu la secci√≥ Eines avall).\n\nLa que vaig acabar escollint era Ludotipia1, i amb aquesta decisi√≥ va quedar fixat:\n\nEl n√∫mero total de cartes: 108 (l‚Äôaplicatiu web d√≥na diverses opcions, i aquesta era la m√©s propera a jocs similars)\nLa plantilla per maquetar les cartes, amb tots els marges necessaris. Teniu una imatge de la plantilla abaix tot i que us recomano que treballeu amb el PDF original per que no tingueu sorpreses amb les mides quan genereu els fitxers finals.\n\n\n\n\nPlantilla Tipia"
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#eines",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#eines",
    "title": "Com fer el teu Cronocartes",
    "section": "Eines",
    "text": "Eines\nEls programes de disseny gr√†fic com Adobe Photoshop i similars acostumen a estar fora del pressupost dels dissenyadors aficionats, pel que la opci√≥ de programari lliure √©s la opci√≥ m√©s econ√≤mica. De fet, s√≥n eines que cobreixen amb escreix les tasques que s‚Äôhan de realitzar. Us passo a continuaci√≥ una relaci√≥ de les eines que he utilitzat i l‚Äô√∫s que li he donat:\n\nScribus √©s una aplicatiu d‚Äôescriptori per autoedici√≥ que he utilitzat per maquetar les cartes. Una de les caracter√≠stiques fonamentals d‚Äôaquesta aplicaci√≥ √©s que permet fer el que es coneix com a ‚Äúmail-merge‚Äù: generar totes les cartes d‚Äôuna manera autom√†tica, r√†pida i senzilla a trav√©s d‚Äôuna sola plantilla i un fitxer de text en format Comma Separated Value (CSV) que cont√© totes les dades de cadascuna de les cartes.\nLibreOffice Calc per editar fulles de c√†lcul i guardar-les en format CSV. La idea √©s generar un fitxer en que cada fila contingui t√≠tol, any i imatge per cada carta. Amb aquest fitxer i la plantilla de maquetaci√≥ es poden generar totes les cartes de la baralla (i no haver d‚Äôanar muntant les cartes una a una). Al ser fitxers de text, la veritat √©s que hi han moltes alternatives per editar CSV: si ets programador, pots utilitzar VS Code amb l‚Äôextensi√≥ Edit CSV.\nGimp, programari lliure d‚Äôedici√≥ gr√†fica, indispensable per manipular les imatges (sobretot escalat, rotacions, crop, manipulaci√≥ de colors i filtrat d‚Äôimatges, ‚Ä¶)\n\nEl sistema operatiu que utilitzo √©s Ubuntu/Linux, per√≤ tot aquest programari t√© versions per altres sistemes operatius."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#events",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#events",
    "title": "Com fer el teu Cronocartes",
    "section": "Events",
    "text": "Events\nJuntament amb la creaci√≥ de les Imatges, el llistat d‚Äôevents √©s el m√©s complicat. Encara que no ho sembli, buscar 100 events hist√≤rics que estiguin documentats i m√©s o menys distribu√Øts en els 2000 anys d‚Äôhist√≤ria pot ser complicat. En el cas del Cronocartes Hist√≤ria de Catalunya vaig tenir en compte els seg√ºents punts:\n\nVolia afegir un punt de complexitat aix√≠ que la majoria de cartes estan concentrades a finals del segle XIX i principis del XX.\nTrobar events del primer mil¬∑leni pot ser un repte per que no hi han gaires refer√®ncies escrites (a excepci√≥ d‚Äôevents clau com guerres, tractats o revoltes).\nHi ha vegades √©n que √©s impossible saber l‚Äôany exacte d‚Äôun event. En aquests casos podeu posar una c. (del llat√≠ circa, aproximadament) abans de l‚Äôany. Una altra alternativa m√©s f√†cil d‚Äôinterpretar pot ser ~.\nSi us costa trobar la data d‚Äôalgun event, opteu per posar la primera refer√®ncia escrita sobre l‚Äôevent (el cas paradigm√†tic del Cronocartes Hist√≤ria de Catalunya √©s el Mat√≥, sabeu de quin any es t√© la primera refer√®ncia? üòú)\nL‚Äôespai pel text de l‚Äôevent √©s redu√Øt (m√©s del que sembla). Haureu de ser creatius per reduir el nombre de car√†cters! No dubteu a utilitzar ordinals en comptes de paraules (per exemple 1r en comptes de Primer). Algunes vegades he hagut de rec√≥rrer a un subt√≠tol (amb un tipus de lletra m√©s petit) en cas que calgu√©s donar una mica m√©s de context, com es mostra en l‚Äôexemple que segueix:\n\n\n\n\nPont del diable, exemple de subt√≠tol"
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#imatges",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#imatges",
    "title": "Com fer el teu Cronocartes",
    "section": "Imatges",
    "text": "Imatges\nPer les imatges de la versi√≥ d‚ÄôHist√≤ria de Catalunya vaig rec√≥rrer a imatges d‚Äôinternet i alguna composici√≥ pr√≤pia. Compteu moltes hores de manipulaci√≥ d‚Äôimatges amb GIMP per acabar d‚Äôajustar-les al format de les cartes. Cal dir que tamb√© √©s una de les parts m√©s creatives del proc√©s, i les possibilitats i eines que dona un programa com GIMP s√≥n innumerables.\nArribats a aquest punt, haureu de vigilar amb els drets d‚Äôautor, especialment si voleu distribuir comercialment el producte. Assegureu-vos que les imatges es poden utilitzar per finalitats comercials. Si teniu dubtes, considereu altres alternatives. En el meu cas, per altres versions estic utilitzant eines d‚Äôintel¬∑lig√®ncia artificial que a m√©s agilitzen substancialment el proc√®s de creaci√≥ d‚Äôimatges a partir d‚Äôun prompt o una altra imatge (o combinaci√≥ d‚Äôimatges). Les eines que he utilitzat s√≥n:\n\nKrea.ai, eina bastant potent que permet generar imatges a partir d‚Äôuna frase o b√© a partir d‚Äôaltres imatges.\nUncrop, eina que permet estendre una imatge (uncrop) i arribar a una certa resoluci√≥ o mida. Molt m√©s √∫til del que pot semblar a simple vista, sobretot per fotografies antigues amb mida redu√Øda. A baix incloc un exemple on la imatge original (esquerra) s‚Äôha modificat per estendre-la per la part superior i generar un sostre (imatge de la dreta).\n\n\n\n\n\n\n\n\nImatge original\nImatge uncropped\n\n\n\n\n\n\n\n\n\nAmbdues eines s√≥n gratu√Øtes, per√≤ tenen funcionalitat limitada. En el meu cas, els l√≠mits d‚Äô√∫s s√≥n m√©s que suficients, per√≤ m‚Äôhe plantejat m√©s d‚Äôun cop concentrar la feina de modificaci√≥ d‚Äôimatges en un mes i subscriure‚Äôm a aquests serveis un parell de mesos per tenir acc√©s a m√©s prestacions que poden ser √∫tils (processat m√©s r√†pid i cap limitaci√≥ en el nombre d‚Äôimatges generades, eliminaci√≥ del fons, ‚Ä¶)."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#automatitzant-el-proc√©s",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#automatitzant-el-proc√©s",
    "title": "Com fer el teu Cronocartes",
    "section": "Automatitzant el proc√©s",
    "text": "Automatitzant el proc√©s\nFer un joc de taula √©s un proc√©s iteratiu, pel que cal tenir un m√®tode que permeti:\n\nRegenerar totes les cartes d‚Äôuna manera r√†pida i √†gil per tal de poder provar f√†cilment v√†ries opcions de disseny i maquetaci√≥.\nTenir un control de les diferents versions que es van fent del mateix. D‚Äôaquesta manera tot el proc√©s de creaci√≥ ser√† reproduible i tra√ßable.\n\nAutomatitzar tot el proc√©s garanteix la consistencia en el disseny de tot el joc i evita els ‚Äúcasos especials‚Äù que acaben sent un malson quan es va modificant el disseny. Sempre que podeu, intenteu pensar com automatitzar el disseny des del principi de la creaci√≥ del joc.\nPer aquest motiu l‚Äôelecci√≥ de les eines √©s critica. Una de les prestacions m√©s important d‚ÄôScribus √©s el fet de poder fer mail-merge: creaci√≥ de totes les cartes a partir d‚Äôuna sola plantilla i un CSV amb les dades de totes les cartes. Aquest proc√©s es pot realitzar amb el plug-in Scribus Generator."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#footnotes",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#footnotes",
    "title": "Com fer el teu Cronocartes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNo tinc cap vincle contractual amb ells ni cobro cap mena de comissi√≥ o ingr√©s de publicitat per part seva.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html",
    "href": "posts/ticket-to-ride-graph-analysis/index.html",
    "title": "Graph theory to model Ticket to ride",
    "section": "",
    "text": "This post contains a basic analysis of the mathematical model behind the board game Ticket to Ride by leveraging graph theory.\nAs you know, ‚ÄúTicket to Ride‚Äù is a game where players aim at completing train routes in the most efficient way possible. The first version of the game is set in the United States of America in the late 19th century during the railroad expansion.\nModeling the game is understood as assessing the distribution of:"
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html#analysis",
    "href": "posts/ticket-to-ride-graph-analysis/index.html#analysis",
    "title": "Graph theory to model Ticket to ride",
    "section": "Analysis",
    "text": "Analysis\nTo do this analysis, we have used following key libraries:\n\nPandas for data loading and processing\nNetworkX for graph processing and above all, to find the shortest path between two stations (nodes).\n\nFirst we will need to install some required libraries as well as import necessary modules\n\n!pip install matplotlib networkx pandas\n\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\n\nThe following cells contain some constants used in the notebook as well as the links to the CSVs that contain the Tracks and routes\n\nTRACK_LENGTH_COLUMN = 'length'\nFROM_COLUMN = 'from'\nTO_COLUMN = 'to'\n\nTRACKS_CSV_FILE = 'Ticket_to_ride - Tracks.csv'\nTICKETS_CSV_FILE = 'Ticket_to_ride - Tickets.csv'\n\nFirst let‚Äôs load the Track CSV, this will be used to visualize the track network as a mathetmatical graph (of nodes/stations and edges/tracks). The CSV contains the two stations for each track as well as their associated color and lenghts (number of wagons required to complete the track)\n\ndf = pd.read_csv(TRACKS_CSV_FILE)\ndf[TRACK_LENGTH_COLUMN] = pd.to_numeric(df[TRACK_LENGTH_COLUMN])\ndf.head()\n\n\n\n\n\n\n\n\nfrom\nto\ncolor\nlength\n\n\n\n\n0\nWashington\nNew York\nblack\n2\n\n\n1\nPittsburgh\nChicago\nblack\n3\n\n\n2\nRaleigh\nNashville\nblack\n3\n\n\n3\nDuluth\nWinnipeg\nblack\n4\n\n\n4\nKansas City\nDenver\nblack\n4\n\n\n\n\n\n\n\nWith this DataFrame we can know useful information such as:\n\nNumber of total tracks\nNumber of wagons required for each color\nTrack distribution for each color\n\nIn addition, we can use the DataFrame to create the graph using the networkx library and make a visual representation of the track network\n\n# Create a directed graph with weighted edges\ngraph = nx.from_pandas_edgelist(df, FROM_COLUMN, TO_COLUMN, edge_attr=TRACK_LENGTH_COLUMN, create_using=nx.Graph())\n\n# Plot the graph\npos = nx.spring_layout(graph, weight=TRACK_LENGTH_COLUMN, seed=4)\nnx.draw(graph, pos, with_labels=True, node_size=400, node_color='skyblue', font_size=9, font_color='black', font_weight='bold', edge_color='grey', linewidths=2)\n\n# Add edge labels showing weights\nedge_labels = nx.get_edge_attributes(graph, TRACK_LENGTH_COLUMN)\nnx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe advantage of modeling the track network as a graph is that we can use already existing libraries to compute the shortest path between two stations (both the passing stations as well as the total length), as in the following example\n\nFROM = 'El Paso'\nDESTINATION = 'Raleigh'\n\nshortest_path = nx.shortest_path(graph, source=FROM, target=DESTINATION, weight=TRACK_LENGTH_COLUMN)\nprint(f'The shortest path goes through these stations: {\" - \".join(shortest_path)}' )\n\nshortest_path_length = nx.shortest_path_length(graph, source=FROM, target=DESTINATION, weight=TRACK_LENGTH_COLUMN)\nprint(f'The length for the shortes path is: {shortest_path_length}')\n\nThe shortest path goes through these stations: El Paso - Dallas - Little Rock - Nashville - Raleigh\nThe length for the shortes path is: 12\n\n\nThe shortest path goes through these stations: El Paso - Dallas - Little Rock - Nashville - Raleigh\nThe length for the shortes path is: 12\nThese methods (specially nx.shortest_path_length) are very relevant to reverse engineer the ticket cards (that give points if a player completes a route). Basically we need to know how the points awarded per card is related to the shortest path.\nTo do so, we will load the Ticket CSV into a DataFrame and compute the shortest path for each card:\n\n# Load the Ticket CSV\ndf_tickets = pd.read_csv(TICKETS_CSV_FILE)\n\n# Create a new column in the dataframe with the shortest path length for each route (i.e. ticket card)\ndf_tickets[TRACK_LENGTH_COLUMN] = df_tickets.apply(lambda r: nx.shortest_path_length(graph, source=r[FROM_COLUMN], target=r[TO_COLUMN], weight=TRACK_LENGTH_COLUMN), axis=1)\n\ndf_tickets.head()\n\n\n\n\n\n\n\n\nfrom\nto\npoints\nlength\n\n\n\n\n0\nLos Angeles\nNew York\n21\n20\n\n\n1\nWinnipeg\nLittle Rock\n11\n11\n\n\n2\nVancouver\nSanta Fe\n13\n13\n\n\n3\nMontreal\nAtlanta\n9\n9\n\n\n4\nToronto\nMiami\n10\n10\n\n\n\n\n\n\n\nAt this point we can realise that the points awarded per route are in fact equivalent (at least on the most cases) with the lenghts (i.e.¬†total number of wagons required to complete the route), which makes sense.\n\nplt.plot(df_tickets[TRACK_LENGTH_COLUMN], df_tickets['points'], 'o')\nplt.xlabel('Shortest route length [number of wagons]')\nplt.ylabel('Ticket card points')\nplt.title('Relationship between Ticket card points and\\nShortest route length')\nplt.show()"
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html#key-takeaways",
    "href": "posts/ticket-to-ride-graph-analysis/index.html#key-takeaways",
    "title": "Graph theory to model Ticket to ride",
    "section": "Key takeaways",
    "text": "Key takeaways\nFrom the analysis, the following takeaways are relevant in case you need to make your own customization of Ticket to ride:\n\nThere are 36 stations\nThere are a total of 100 tracks (double tracks are considered independent tracks)\nEach color (except grey) needs a total of 27 wagons to complete their tracks\nEach color (except grey) has a total number of 7 tracks, and their distribution is: 6-5-4-4-3-3-2, except for green and white, for which their distribution is: 6-5-5-4-3-2-2.\nTo the largest possible extent, the colors are homogeneously distributed on the territory (which makes sense in order to balance the game).\nThere are 30 ticket cards\nThe points for each ticket card represent the shortest path length (i.e.¬†minimum number of wagons required to complete the card), with some small exceptions."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "",
    "text": "GNSS Expert | Tech Transfer and Innovation | Software Development"
  },
  {
    "objectID": "index.html#expertise",
    "href": "index.html#expertise",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "Expertise",
    "text": "Expertise\nMy technological expertise covers\n\nGlobal Navigation Satellite Systems technology, from algorithm definition to implementation. My main expertise covers Navigation algorithms as well as ionospheric monitoring using GNSS.\nGNSS data analysis\nTerrestrial and Alternate Position, Navigation and Timing (PNT) systems (e.g.¬†Wi-Fi Round Trip Time and other range-based systems such as 5G and Ultrawide-Band)\nSoftware development (from proof-of-concept to production based systems based on Continuous Integration and Continuous Deployment)\n\nOther relevant skills\n\nTechnology transfer and Innovation\nEU tender and grant proposal preparation and evaluation\nProject management"
  },
  {
    "objectID": "index.html#short-bio",
    "href": "index.html#short-bio",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "Short bio",
    "text": "Short bio\nI am a GNSS Research Engineer with over 20 years of experience. I holds a BSc in Telecommunication Engineering (1999) and a PhD in GNSS Data Processing for Ionospheric Monitoring (2004) from the Polytechnic University of Catalonia (Spain).\nI have a diverse career spanning academia and industry, and have held key positions at renowned institutions, including:\n\nRokubun: CTO and co-founder position of Rokubun\nJPL/NASA: Technologist positgion, doing GNSS data processing for navigation, geodesy and orbit determination, as well as implementing physical models for the next generation GPS control segment\nStarlab Barcelona: Space Program and Area Manager, executing projects related to GNSS data processing for Earth observation as well as GNSS receiver development.\nDLR: Scientific saff at the GNSS Technology and Navigation group of the German Aerospace Center (DLR), developing GNSS systems for LEO applications.\nUniversity of Kyoto: Post-doctoral position for one year in the Research Center of Sustainable Humanosphere (RISH) on data combination for ionospheric monitoring,\n\nI am also an associate editor in the GPS Solutions journal."
  },
  {
    "objectID": "games/status.html",
    "href": "games/status.html",
    "title": "Desenvolupament de jocs",
    "section": "",
    "text": "L‚Äôestat de desenvolupament de les diferents idees o projectes relacionats amb els jocs de taula que tinc entre mans est√† resumit en aquest diagrama que teniu abaix i, que, com no podia ser d‚Äôuna altra manera, l‚Äôhe fet amb Python i he incrustat el codi aqu√≠ abaix per si teniu curiositat.\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\n\nPROJECT_DATA = [\n  (\"Aventureiros √≥ tren, Galicia\", 0.4, \"Playtesting\"),\n  (\"C√≥digo Segredo\", 0.9, \"Primera versi√≥n completa, PnP na boardgamegeek\"),\n  (\"Cronocartas Historia de Galicia\", 0.7, \"En preproducci√≥n\"),\n  (\"Cronocartes Hist√≤ria de Catalunya\", 1.0, \"Disponible\"),\n\n]\n\n# Create a figure\nfig = go.Figure()\n\n# Set up bar parameters\nbar_width = 0.35\nbar_height = 0.1\ngap_between_bars = 0.05\nnum_bars = len(PROJECT_DATA)\n\n# Calculate bar positions and widths\nbar_positions = np.arange(num_bars) * (bar_width + gap_between_bars) + bar_width / 2\nbar_widths = [bar_width] * num_bars\n\n# Create foreground bars with rounded corners and slightly smaller width\nfor i in range(num_bars):\n    completion_rate = PROJECT_DATA[i][1]\n    hover_text = PROJECT_DATA[i][2]\n\n    fig.add_trace(go.Bar(\n        x=[completion_rate],\n        y=[bar_positions[i]],\n        width=bar_widths[i] * 0.8,\n        orientation='h',\n        marker=dict(color='#14a2ff', line=dict(width=1, color='#0072bd'), cornerradius=10),\n        hovertext=hover_text,\n        hoverinfo='text',\n    ))\n\n# Add project names within bars, left-justified\nfor i in range(num_bars):\n    project_name = PROJECT_DATA[i][0]\n    x = PROJECT_DATA[i][1]\n    fig.add_annotation(\n        x = 0.02,\n        y = bar_positions[i],\n        text = project_name,\n        showarrow = False,\n        xanchor=\"left\",\n        font=dict(color=\"white\", size=11),\n    )\n\n# Set up plot appearance\nfig.update_layout(\n    xaxis=dict(range=[0, 1], tickvals=[0, 0.25, 0.5, 0.75, 1], ticktext=['0%', '25%', '50%', '75%', '100%']),\n    yaxis=None,\n    #margin=dict(l=0, r=0, b=0, t=30),\n    showlegend=False,\n    autosize=False,\n    width=500,\n    height=200,\n)\nfig.update_yaxes(showticklabels=False)\nfig.layout.xaxis.fixedrange = True\nfig.layout.yaxis.fixedrange = True\nfig.select_annotations\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "games/cronocartes/index.html",
    "href": "games/cronocartes/index.html",
    "title": "Cronocartes Hist√≤ria de Catalunya",
    "section": "",
    "text": "Descobreix la hist√≤ria de Catalunya a trav√©s del joc de cartes, Cronocartes, on historiadors rivals competiran per demostrar el seu coneixement de la hist√≤ria de Catalunya.\nCronocartes √©s un joc de cartes r√†pid i familiar: nom√©s necessiteu 30 segons per aprendre a jugar i les partides no acostumen a durar m√©s de 20 minuts. Molts adults tenen por de jugar per vergonya a una suposada ignor√†ncia en temes hist√≤rics, per√≤ ja veureu com no n‚Äôhi ha per a tant! Us divertireu!"
  },
  {
    "objectID": "games/cronocartes/index.html#com-es-juga",
    "href": "games/cronocartes/index.html#com-es-juga",
    "title": "Cronocartes Hist√≤ria de Catalunya",
    "section": "Com es juga",
    "text": "Com es juga\nTrobareu les instruccions en una de les cartes de la mateixa baralla:\n\n\n\n\n\n\n\n\n\n\n\nPreparaci√≥\nSeguiu aquests passos per preparar la partida:\n\nMescleu b√© les cartes i repartiu-ne 7 a cada historiador cara avall (per la cara que no mostra l‚Äôany).\nUn cop repartides les cartes, preneu la carta superior de la pila de cartes restant i poseu-la cara amunt de tal manera que l‚Äôany ha de quedar a la vista. Aquesta ser√† la carta inicial de la l√≠nia temporal.\n\nJa esteu a punt per comen√ßar!\n\n\nEl teu torn\nEn el teu torn haur√†s d‚Äôagafar una de les teves cartes que estan cara avall i col¬∑locar-la a la l√≠nia temporal, on creguis que respecta l‚Äôordre cronol√≤gic. Despr√©s d‚Äôhaver-te decidit, gira la carta. En aquest moment poden passar dues coses:\n\nLa carta respecta l‚Äôordre cronol√≤gic (ben fet!üëè), deixa la carta a la l√≠nia temporal.\nLa carta no respecta l‚Äôordre cronol√≤gic (t‚Äôhas equivocat!üò•), descarta la carta i pre-ne una altra de la pila de cartes.\n\nUn cop hagis resolt un d‚Äôaquests casos, passa el torn al/la seg√ºent historiador/a.\n\n\nFinal del joc\nEn el moment en qu√® un/a historiador/a col¬∑loca correctament la seva √∫ltima carta, es dispara el final del joc i s‚Äôacaba la ronda (per tal que tots els/les historiadors/es hagin jugat el mateix nombre de torns).\nAl final de l‚Äô√∫ltima ronda, el jugador que s‚Äôhagi quedat sense cartes guanya la partida!\nEn cas que a l‚Äô√∫ltima ronda hagi hagut un empat i m√©s d‚Äôun/a historiador/a hagi acabat les seves cartes, la vict√≤ria es decidir√† per mort sobtada: s‚Äôaniran jugant cartes i els/les finalistes quedaran eliminats en cas de col¬∑locar incorrectament una carta.\n\n\nVariant avan√ßada\nSi voleu fer el joc una mica m√©s dif√≠cil, en comptes de descartar la carta en cas d‚Äôequivocaci√≥, col¬∑loqueu-la correctament a la l√≠nia temporal.\n\n\nEstrat√®gia\nUn consell: tot i que esteu temptats a col¬∑locar primer les cartes que sabeu, heu de fer precisament el contrari! Jugueu primer les que s√≥n m√©s dubtoses. Si no ho feu aix√≠, a mesura que la l√≠nia temporal es vagi poblant de cartes, us ser√† m√©s dif√≠cil afinar l‚Äôany i la probabilitat d‚Äôequivocar-se ser√† m√©s alta."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Catal√†GalegoEnglish\n\n\nEls que es volen adre√ßar a mi, normalment ho fan pel meu nom, Miquel. Alguns, m√©s formals, utilitzen el meu primer cognom, Garcia.\nSe‚Äôm coneix per ser un enginyer de sistemes de navegaci√≥ (GPS pels profans en el tema), programador, somiatruites, aprenent etern i sobretot friqui, molt friqui.\n\n\nAqueles que queren dirixirse a min normalmente o fan polo meu nome, Miquel. Alg√∫ns, m√°is formais, usan o meu primeiro apelido, Garcia.\nCo√±√©cenme por ser un enxe√±eiro de sistemas de navegaci√≥n (GPS para os profanos no tema), programador, so√±ador, aprendiz eterno e sobre todo friqui, moi friqui.\n\n\nThose who want to address me usually do so by my name, Miquel. Some, more formally, use my first surname, Garcia.\nI am known to be a navigation systems engineer (GPS for those unfamiliar with the subject), programmer, dreamer, eternal learner, and a geek.\n\n\n\nEm podeu trobar per aqui\n   \nSi voleu seguir el canal de creaci√≥ de jocs que tinc a Telegram o WhatsApp, aqu√≠ us deixo els links:"
  },
  {
    "objectID": "games/torneig_cronocartes.html",
    "href": "games/torneig_cronocartes.html",
    "title": "CronoTorneig",
    "section": "",
    "text": "Aquesta p√†gina inclou notes, idees i instruccions per muntar un torneig de Cronocartes. En particular, es defineixen regles de puntuaci√≥, desempats i organitzaci√≥ de rondes."
  },
  {
    "objectID": "games/torneig_cronocartes.html#organitzaci√≥-b√†sica",
    "href": "games/torneig_cronocartes.html#organitzaci√≥-b√†sica",
    "title": "CronoTorneig",
    "section": "Organitzaci√≥ b√†sica",
    "text": "Organitzaci√≥ b√†sica\nPer un torneig en el qual participen 16 historiadors:\n\nA cada partida hi competiran 4 historiadors (necessiteu doncs 4 taules)\nUn cop hi hagi guanyador, la partida continuar√† fins que tots els jugadors col¬∑loquin les seves cartes. Aquest pas √©s important perqu√® cal saber qui queda 1r, 2n, 3r i 4t. Les puntuacions de cada historiador seran les seg√ºents:\n\n1r classificat: 5 punts\n2n classificat: 3 punts\n3r classificat: 2 punts\n4t classificat: 1 punts\n\nEls empats per qualsevol posici√≥ es desfaran per mort sobtada: cada historiador rebr√† una carta i quedar√† eliminat en cas que s‚Äôequivoqui al posar la carta a la l√≠nia temporal ja existent de la partida en curs.\nEl torneig s‚Äôestructurar√† en un total de 5 rondes distribu√Ødes de la seg√ºent manera:\n\nRondes classificat√≤ries: 3 rondes en format su√≠s. D‚Äôuna ronda a la seg√ºent, els historiadors s‚Äôagruparan per puntuaci√≥: els 4 primers aniran a una taula, els 4 segons a una altra i aix√≠ successivament. D‚Äôaquesta manera, si un historiador ha tingut una mala partida, encara tindr√† opcions de guanyar.\nRondes eliminat√≤ries: semi-final i final. Per la semi-final, s‚Äôorganitzaran les taules de manera ponderada: la primera taula estar√† constitu√Øda pel 1r, 4t, 5√® i 8√® classificat, i a la segona taula hi seran el 2n, 3r, 6√® i 7√®. El motiu √©s per tenir dues taules homog√®nies en tant que nivell dels historiadors. Finalment, la taula de la ronda final estar√† constitu√Øda pels dos primers classificats de la taula de semi-finals."
  },
  {
    "objectID": "games/torneig_cronocartes.html#empat-a-la-gran-final",
    "href": "games/torneig_cronocartes.html#empat-a-la-gran-final",
    "title": "CronoTorneig",
    "section": "Empat a la gran final",
    "text": "Empat a la gran final\nEn cas que hi hagi un empat entre els finalistes i totes les cartes s‚Äôhagin esgotat (com va passar a la final del 2n torneig Cronomaster del Museu d‚ÄôHist√≤ria de Catalunya), aqu√≠ teniu algunes idees:\n\nFeu una partida al Cronocartas Historia de Galicia\nEliminat√≤ria per batalla de Burritos\nLlen√ßar una moneda\nPartida d‚Äôescacs\n\nSi tot aix√≤ falla, sempre quedar√† una bona llen√ßada de daus. üòú"
  },
  {
    "objectID": "games/torneig_cronocartes.html#altres-consideracions",
    "href": "games/torneig_cronocartes.html#altres-consideracions",
    "title": "CronoTorneig",
    "section": "Altres consideracions",
    "text": "Altres consideracions\n\n‚ÄúHandicap‚Äù. Es repartiran cartes addicionals a guanyadors d‚Äôedicions anteriors del torneig: una carta addicional per cada edici√≥ anterior en qu√® ha resultat guanyador. Exemple: En Sever√≠ ha guanyat el 1r i el 2n torneigs de Cronocartes, per tant, a la 3a edici√≥ del torneig comen√ßar√† amb 9 cartes, en comptes de 7.\nPartides de tres historiadors: Repartiu 8 cartes en comptes de 7 per mantenir el nivell de dificultat similar que en partides de 4 historiadors amb 7 cartes. Per exemple, amb 4 historiadors hi ha 28 esdeveniments (\\(4 \\cdot 7 = 28\\)), mentre que amb 3 historiadors, si es reparteixen 8 cartes, n‚Äôhi haur√† 24 (\\(3 \\cdot 8 = 24\\))."
  },
  {
    "objectID": "games/torneig_cronocartes.html#agra√Øments",
    "href": "games/torneig_cronocartes.html#agra√Øments",
    "title": "CronoTorneig",
    "section": "Agra√Øments",
    "text": "Agra√Øments\nGr√†cies a Monica Mallo i Guillem Guasch per la revisi√≥ del contingut d‚Äôaquesta p√†gina."
  },
  {
    "objectID": "games/cronocartas/index.html",
    "href": "games/cronocartas/index.html",
    "title": "Cronocartas Historia de Galicia",
    "section": "",
    "text": "Descubre a historia de Catalu√±a xogando a Cronocartes, un xogo de cartas onde historiadores rivais competir√°n para demostrar os seus co√±ecementos sobre a historia de Galiza.\nCronocartas √© un xogo r√°pido e familiar: en s√≥ 30 segundos aprender√°s a xogar e as partidas adoitan durar menos de 20 minutos. Moitos adultos te√±en medo de xogar por vergo√±a a unha suposta ignorancia en temas hist√≥ricos, pero xa ver√°s que non hai nada de que preocuparse! ¬°Divertir√©desvos!"
  },
  {
    "objectID": "games/cronocartas/index.html#como-se-xoga",
    "href": "games/cronocartas/index.html#como-se-xoga",
    "title": "Cronocartas Historia de Galicia",
    "section": "Como se xoga",
    "text": "Como se xoga\nAtoparedes as instruci√≥ns nunha das cartas da mesma baralla:"
  },
  {
    "objectID": "games/cronocartas/index.html#preparaci√≥n",
    "href": "games/cronocartas/index.html#preparaci√≥n",
    "title": "Cronocartas Historia de Galicia",
    "section": "Preparaci√≥n",
    "text": "Preparaci√≥n\nSigue estes pasos para preparar a partida:\n\nBaralla ben as cartas e reparte 7 a cada historiador boca abaixo (pola cara que non mostra o ano).\nUnha vez repartidas as cartas, colle a carta superior da pila de cartas restante e pona boca arriba de tal maneira que o ano quede √° vista. Esta ser√° a carta inicial da li√±a temporal.\n\n¬°Xa est√°s a punto para comezar!"
  },
  {
    "objectID": "games/cronocartas/index.html#final-do-xogo",
    "href": "games/cronocartas/index.html#final-do-xogo",
    "title": "Cronocartas Historia de Galicia",
    "section": "Final do xogo",
    "text": "Final do xogo\nO momento en que un/a historiador/a coloca correctamente a s√∫a √∫ltima carta dispara o final do xogo. S√©guese xogando ate que acabe a ronda (para que todos os/as historiadores/as poidan xogar o mesmo n√∫mero de quendas).\nAo final da √∫ltima ronda, o xogador que se quedou sen cartas ga√±a a partida!\nNo caso de que na √∫ltima ronda haxa un empate, e m√°is dun/a historiador/a haxa acabado as s√∫as cartas, a vitoria decidirase por morte s√∫bita: ir√°n xogando cartas e os/as finalistas quedar√°n eliminados no caso de colocar incorrectamente unha carta."
  },
  {
    "objectID": "games/cronocartas/index.html#variante-avanzada",
    "href": "games/cronocartas/index.html#variante-avanzada",
    "title": "Cronocartas Historia de Galicia",
    "section": "Variante avanzada",
    "text": "Variante avanzada\nSe queres facer o xogo un pouco m√°is dif√≠cil, en vez de descartar a carta no caso de equivocaci√≥n, col√≥caa correctamente na li√±a temporal."
  },
  {
    "objectID": "games/cronocartas/index.html#estratexia",
    "href": "games/cronocartas/index.html#estratexia",
    "title": "Cronocartas Historia de Galicia",
    "section": "Estratexia",
    "text": "Estratexia\nUn consello: a√≠nda que esteas tentado a colocar primeiro as cartas que sabes, tes que facer precisamente o contrario! Xoga primeiro as que son m√°is dubidosas. Se non o fas as√≠, √° medida que a li√±a temporal v√°iase poboando de cartas, serache m√°is dif√≠cil afinar o ano e a probabilidade de equivocarse ser√° m√°is alta."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html",
    "href": "posts/parquet_format_for_gnss_measurements/index.html",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "",
    "text": "Following up on a previous idea regarding the sheer volume of the GNSS measurements generated by CORS1 networks, this post explores a potential binary storage of GNSS measurements (rather than text) leveraging the Apache parquet format. This format is highly used in Artificial Intelligence and Machine Learning applications, which is being also adopted in GNSS for e.g.¬†prediction of precise product (orbits and clocks), see also Siemuri et al. (2022) or Mao et al. (2024). By transitioning from text-based storage to a binary format like Parquet, we can unlock significant speed improvements in GNSS data processing pipelines, eliminating the need for time-consuming text parsing.\nFrom this post, you can expect the following:"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#converting-rinex-to-parquet",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#converting-rinex-to-parquet",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Converting RINEX to parquet",
    "text": "Converting RINEX to parquet\nConverting RINEX files to the Parquet format involves a two-step process:\n\nParsing the RINEX data: We leverage the roktools library to efficiently parse the RINEX file and organize the data into a structured format.\nStorage with pandas: Once parsed, the data is seamlessly converted into a pandas DataFrame. Pandas, a popular Python library for data manipulation, offers a built-in function for writing DataFrames directly to Parquet format.\n\nThis approach simplifies the conversion process and is shown in the code snippet below:\n\nimport tempfile\nimport pandas as pd\nfrom roktools import rinex\n\n# we use Context Manager to store the parquet file in a\n# temporary file that will be cleaned up automatically\n# after loading it\nwith tempfile.NamedTemporaryFile() as fh:\n\n    rinex.to_parquet(['SUN600SWE_S_20241312200_01M_01S_MO.rnx'], output_filename=fh.name)\n\n    # Rewind temporary file\n    fh.seek(0)\n\n    # Load the parquet file into a DataFrame\n    df = pd.read_parquet(fh.name)\n\n# Print preview of the DataFrame\ndf.head()\n\n\n\n\n\n\n\n\nepoch\nconstellation\nsat\nchannel\nsignal\nrange\nphase\ndoppler\nsnr\nslip\nstation\n\n\n\n\n0\n2024-05-10 22:00:00\nG\nG04\n1C\nG041C\n2.462384e+07\n1.293992e+08\n-3646.251\n35.0\n0\nsun6\n\n\n1\n2024-05-10 22:00:00\nG\nG04\n2W\nG042W\n2.462385e+07\n1.008308e+08\nNaN\n20.0\n0\nsun6\n\n\n2\n2024-05-10 22:00:00\nG\nG04\n2X\nG042X\n2.462385e+07\n1.008308e+08\nNaN\n41.0\n0\nsun6\n\n\n3\n2024-05-10 22:00:00\nG\nG04\n5X\nG045X\n2.462385e+07\n9.662954e+07\nNaN\n45.0\n0\nsun6\n\n\n4\n2024-05-10 22:00:00\nG\nG04\n1X\nG041X\n2.462384e+07\n1.293995e+08\nNaN\n37.0\n0\nsun6\n\n\n\n\n\n\n\nAs you can see, the data has a columnar layout with the following fields:\n\nepoch of the measurement (corresponds to the RINEX epoch of the measurements)\nconstellation: single character with the constellation (RINEX convention, e.g.¬†R for Glonass, G for GPS, ‚Ä¶)\nsat: Three character satellite identifier. First letter corresponds to the constellation and the last two characters corresponds to the satellite number (as defined by the RINEX format).\nchannel: two character description of the tracking channel. The values from this field correspond to the last two characters of the three-character RINEX channel code. The first character (the observable type) has been dropped because all measurements associated to this tracking channel (code, phase, Doppler and C/N0) are placed in the same row. This layout makes it straightforward to compute observables such as code-minus-carrier.\nsignal: Union of the satellite and channel fields. Albeit this may seem a redundant field, some optimization considerations make this field useful, as explained in the section below.\nrange: Pseudorange in meters\nphase: Carrier-phase in cycles\ndoppler: Doppler observables expressed in cycles per second\nsnr: C/N0 expressed in dB-Hz\nslip: Cycle slip / loss of lock field of the RINEX format.\nstation: Name of the station that recorded the measurements. Having this field may allow to have observables from multiple stations in the same parquet file."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-compute-the-code-minus-carrier-observable",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-compute-the-code-minus-carrier-observable",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: compute the code-minus-carrier observable",
    "text": "Tip: compute the code-minus-carrier observable\nWith the column layout proposed for the parquet format, extracting observables such as the CMC (code-minus-carrier), that use data for the same tracking channel, becomes straightforward:\n\nimport matplotlib.pyplot as plt\n\nsignal = 'G041C'\n\n# Filter the data to extract only the desired signal\ndf_signal = df[df['signal'] == signal]\n\n# Elapsed time\nt0 = df_signal['epoch'].loc[0]\nx = (df_signal['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# CMC (conversion from cycles to meters is required)\nwavelength_l1 = 299792458/(154.0*10.23e6)\ny = df_signal['range'] - df_signal['phase'] * wavelength_l1\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('Code-minus-carrier [m]')\nplt.title(f'Code-minus-carrier combination for signal {signal}')\nplt.plot(x, y, '.-')\n\n\n\n\nExample of code-minus-carrier combination computation"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-groupby-to-efficiently-process-data",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-groupby-to-efficiently-process-data",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: use groupby to efficiently process data",
    "text": "Tip: use groupby to efficiently process data\nUsually, a GNSS analyst processes the data on a satellite-station (e.g.¬†link) basis or on a signal-basis (satellite and tracking channel). Examples are:\n\nDetection of code outliers or single-frequency cycle slip detection based on jumps in the CMC combination.\nComputation of Rate of Total Electron Content Index (ROTI) for scintillation monitoring (Pi et al. (1997))\nComputation of multi-channel observations such as ionospheric free or geometry free combinations\n\nIn those cases, when using pandas, the groupby strategy becomes in handy to quickly work on a signal-per-signal basis in a very efficient manner\n\n\n\n\n\n\nAvoid for loops\n\n\n\nLeveraging techniques like groupby enables efficient vectorized operations, significantly outperforming slow Python for loops.\n\n\nAs an example, let‚Äôs compute the first derivate of the phase (for all signals) and compare it with the Doppler observable (please note that no for has been used in the code):\n\n# Compute the time derivative of the phase\ndf['d_phase'] = df.groupby('signal')['phase'].diff()\n\n\n\nCode\nsignal = 'G041C'\n\ndf_signal = df[df['signal'] == signal]\n\n# Time elapsed\nt0 = df_signal['epoch'].loc[0]\nx = (df_signal['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('Doppler and phase time derivative [cycles/s]')\nplt.title(f'Time derivative of signal {signal}')\nplt.plot(x, df_signal['d_phase'], '.-', label=\"d phase/dt\")\nplt.plot(x, -df_signal['doppler'], '.-', label=\"Doppler\")\nplt.legend(loc=\"lower right\")\n\n\n\n\n\nTime derivative of the carrier phase"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-merge-to-compute-multifrequency-observables",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-merge-to-compute-multifrequency-observables",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: use merge to compute multifrequency observables",
    "text": "Tip: use merge to compute multifrequency observables\nAnother powerful method to process GNSS data, specially when trying to combine data from different signal, is the database-inherited merge method. This allows to join sections of the DataFrame and quickly build dual-frequency combinations. To illustrate this, let‚Äôs compute the geometry-free (or ionospheric) combination.\n\nchannel_a = '1X'\nchannel_b = '5X'\n\n# Filter DataFrames for each signal\n# Using a mask allows accessing filtered data directly\n# without creating new DataFrames\nmask_a = df['channel'] == channel_a\nmask_b = df['channel'] == channel_b\n\ndf_a = df[mask_a]\ndf_b = df[mask_b]\n\n# Wavelenghts will be required to convert from cycles to meters\nwavelength_a = 299792458/(154.0*10.23e6)\nwavelength_b = 299792458/(115.0*10.23e6)\n\n# Merge on 'time' and calculate the difference\ndf_merged = df_a.merge(df_b, on=['epoch', 'sat'], suffixes=('_a', '_b'))\ndf_merged['li_m'] = df_merged['phase_a'] * wavelength_a - df_merged['phase_b'] *wavelength_b\n\n\n\nCode\nsat = 'E04'\n\ndf_sat = df_merged[df_merged['sat'] == sat]\n\n# Time elapsed\nt0 = df_sat['epoch'].iloc[:1].values[0]\nx = (df_sat['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('LI [m]')\nplt.title(f'Geometry free combination of phases for {channel_a} and {channel_b} ({sat})')\nplt.plot(x, df_sat['li_m'], '.-')\n\n\n\n\n\nExample of geometry free combination (LI)\n\n\n\n\nThe primary limitation of this approach lies in the additional memory required to store the filtering masks. However, this overhead is significantly less substantial compared to creating entirely new DataFrames for each signal. The mask-based approach operates on references to the original DataFrame, avoiding the creation of new data structures.\nAs an alternative to the merge operation, the groupby function can be employed to group data by epoch and subsequently identify the specific channels within each group for combination."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#why-the-constellation-and-signal-fields-enhance-efficiency",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#why-the-constellation-and-signal-fields-enhance-efficiency",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Why the constellation and signal fields enhance efficiency?",
    "text": "Why the constellation and signal fields enhance efficiency?\nThe inclusion of constellation and signal fields within the DataFrame, albeit they may seem redudant, significantly accelerates data processing and analysis by streamlining operations and reducing computational overhead in the following way:\n\nOptimized groupby operations: The signal field directly identifies unique signal types, eliminating the need for complex groupby operations involving multiple fields like ['sat', 'channel']. This simplification leads to substantial performance gains, as groupby operations on single fields are considerably faster.\nEfficient constellation-level analysis: The constellation field provides direct access to constellation information, bypassing the need for time-consuming string manipulation and filtering of the sat field. This enables efficient constellation-level operations, such as counting satellites per constellation or calculating constellation-specific statistics.\n\nBy incorporating these dedicated fields, the DataFrame becomes more efficient and versatile, enabling swift and accurate analysis of GNSS data."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#a-note-on-file-size",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#a-note-on-file-size",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "A note on file size",
    "text": "A note on file size\nDespite the convenience of using parquet files for data processing and the substantial increase in the loading speed (plus the possibility of distributed processing when using Apache Spark instances), in terms of storage it stilss falls short competing with the de-facto standard Hatanaka + Gzip combo.\nTaking as an example the RINEX file ACSO00XXX_R_20241310000_01D_01S_MO.rnx (ACSO station, data for 1 day at 1 second interval, multi-constellation, multi-frequency), the different file sizes using various compression formats are shown in the table below:\n\n\n\ncompression\nsize\n\n\n\n\nUncompressed RINEX file (rnx)\n428 MB\n\n\nHatanaka + Gzip\n37 MB\n\n\nParquet\n141 MB\n\n\nParquet + Gzip\n107 MB\n\n\n\nIn the best case, the size of the compressed parquet is usually 3 times larger than the Hatanaka + Gzip combo, however there may be still some room for improvement if data within the file is organized to exploit parquet features such as run length encoding or RLE.\nSpecifically, RLE can significantly compress GNSS data containing repetitive sequences, such as timestamps that appear multiple times within the file. By reorganizing data within the Parquet file to exploit such features, it may be possible to achieve a more favorable compression ratio."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#conclusions",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#conclusions",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the analyzed data, several key observations emerge:\n\nEase of use:\n\nSimplified Preprocessing: Parquet files offer a streamlined approach to reading GNSS measurements compared to RINEX. The complex multi-step process involving gzip decompression, Hatanaka decompression, and parsing to binary is significantly simplified.\nDirect Integration: Parquet files can be directly downloaded and seamlessly integrated into data analysis frameworks like Pandas, eliminating the need for extensive preprocessing.\n\nData Volume: The combination of Hatanaka and Zip compression still offers a much smaller file size than parquet (on a ratio of 1 to 3), but using features such as run-length encoding (REL) may help reducing the parquet file size."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#use-of-ai",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#use-of-ai",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial Intelligence has been used to polish some text styling and correct some typos. Ideation and data processing has been done by the author."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#footnotes",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#footnotes",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nContinuously Operating Receiving Station‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/codigo-segredo/codigo_segredo.html",
    "href": "posts/codigo-segredo/codigo_segredo.html",
    "title": "Traduci√≥n do xogo ‚ÄúC√≥digo Secreto‚Äù √≥ galego",
    "section": "",
    "text": "Seguro que moitos de v√≥s co√±ecedes o xogo de mesa ‚ÄúC√≥digo Secreto‚Äù, no que dous equipos de esp√≠as tratan de localizar os seus axentes de campo usando palabras. Como √© unha carreira para ver que equipo completa antes esta tarefa, √© crucial usar pistas que agrupen ou asocien m√∫ltiples palabras clave que identifican os nosos axentes de campo.\n\n\n\nC√≥digo Segredo\n\n\n\nSaberedes seguramente que existen variantes de este xogo en diversos idiomas, a castel√°n por suposto, pero tam√©n hai a versi√≥n comercial en catal√°n. Como non vin a versi√≥n en galego me aventurei a facela, e neste periplo aprend√≠n un par de ideas que quizais podan ser √∫tiles a quen queira crear expansi√≥ns a outros idiomas como o √©uscaro, suahili ou casaco. Por que non nos enganemos, traducir o ‚ÄúC√≥digo Segredo‚Äù non √© simplemente traducir literalmente as palabras: hai que trasladar tam√©n o contexto das palabras. Cando te po√±as a traballar, ten en conta estas ideas:\n\nPolisemia e homograf√≠a. En ‚ÄúC√≥digo Segredo‚Äù √© crucial que haxan palabras con m√∫ltiples significados para que os xogadores podan buscar asociaci√≥ns de termos. O problema √© que en linguas que derivan da mesma ra√≠z (por exemplo o lat√≠n), os varios significados que pode agrupar una palabra nun idioma p√≥dense reducir noutro. Por exemplo, ‚Äútaula‚Äù en catal√°n significa mesa e t√°boa (en galego), pero a voz galega ‚Äúmesa‚Äù perde o significado de ‚Äút√°boa‚Äù, as√≠ que teremos que buscar una alternativa.\nContexto e termos culturais. Si tedes a versi√≥n catalana do xogo, atoparedes termos como ‚ÄúMontserrat‚Äù, ‚Äúcaganer‚Äù ou ‚Äúti√≥‚Äù, termos que levan un vencello moi forte ca cultura catalana pero que non existen en galego. A mais, usar termos culturalmente ligados a lingua far√° que a experiencia de xogo sexa moito mais divertida. No caso galego, p√≥dese usar alternativas como ‚ÄúAncares‚Äù, ‚ÄúTerra Ch√°‚Äù, ‚ÄúLobishome‚Äù ou ‚ÄúSanta Compa√±a‚Äù.\n\n\n\n\nMostra de cartas de palabras\n\n\n¬øTedes algunha outra idea sobre este tema que queirades propo√±er?\nDeixo aqu√≠ a ligaz√≥n do ‚ÄúPrint and Play‚Äù para que podades imprimir vos mesmos as cartas de palabras e xogar a versi√≥n galega (s√≥ precisades una copia do xogo en calquera outro idioma dispo√±√≠bel)."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html",
    "href": "posts/gnss_data_data_volume/index.html",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "",
    "text": "For those of you who work with GNSS data processing, have you ever wondered how much data do servers such as CDDIS or EUREF, who store historic GNSS raw measurement data, need to handle?\nIf your use case is to perform a positioning session for a single receiver maybe not, but I bet the situation is different if you are building GNSS products (e.g.¬†estimation of GNSS satellite orbits and clocks, ionospheric monitoring, ‚Ä¶), where a worldwide distribution of receivers (and data) is not only desirable but mandatory.\nIn order to answer this question, I have implemented a script that lists the files stored in the GNSS raw measurement data folders provided CDDIS server. In particular, I selected the daily files with 30 seconds sampling rate. In order not to overwhelm the servers, just one day for each year (January 1st) was processed. Then, the code included below in this post has been used to plot the number of (unique) stations1 as well as the median file size. The total folder size has been also computed.\nInitially, one may think that the volume increase could be due to two factors:\nCode\nimport datetime\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfiles = glob.glob('./data/folder_*.txt')\n\n# List of all dataframes\ndfs = []\n\n# load the data files\nfor f in files:\n\n    date = datetime.datetime.strptime(f, './data/folder_%Y_%j.txt')\n    data = pd.read_csv(f, delimiter=\"\\s+\", names=['file', 'size'])\n    data['date'] = date\n    data['station'] = data['file'].str[:4].str.lower()\n    dfs.append(data)\n\n# Aggregate all dataframes\ndf = pd.concat(dfs)\n\nstats = df.groupby('date').agg(\n    n_stations=('station', lambda x: x.nunique()),\n    median_size=('size', 'median'),\n    total_size=('size', 'sum'))\n\n\nfig, ax1 = plt.subplots(figsize=(8, 4))\n\n# colors\nblue = '#0072bd'\nred = '#a2142f'\n\nplt.title(\"Evolution of GNSS data folder size\\nCDDIS network (daily 30s data files)\")\n\nax1.plot(stats.index, stats['n_stations'], label=\"Number of stations\", c=blue)\n\nax2 = ax1.twinx()\nax2.plot(stats.index, stats['median_size'] / 1.0e6, label=\"Median file size [Mb]\", linestyle='--', c= red)\nax2.plot(stats.index, stats['total_size'] / 1.0e9, label=\"Total folder size [Gb]\", c= red)\n\nax1.set_ylabel('Number of stations [count]', color=blue)\nax2.set_ylabel('File size [Mb] or Folder size [Gb]', color=red)\nax1.tick_params(axis='y', colors=blue)\nax2.tick_params(axis='y', colors=red)\n\nax1.legend(loc='upper left')\nax2.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nGNSS data volume over years for CDDIS network\nBut looking at the figure is clear that the main driving factor for the volume size increase is the larger file size: note that from 2012 the number of stations has stabilized and yet the total size has suffered a large increase (following the trend of the file size). Note also that most of the folders contain duplicated data: data from the same station might be stored in both RINEX v2 and v3 (althought this may change in the future, as V2 support is winding down).\nIn order to illustrate the causes of the file size increase, let‚Äôs take a particular station (EBRE, Observatori de l‚ÄôEbre, Spain) as an example. All files studied are compressed using Hatanaka and binary zip. The constellations and bands tracked in 4 particular files in the past are summarized in the following table:\nIt is clear then that the qualitative jump between 2012 to 2016 is caused by the recording of additional constellations (most notably Galileo and Beidou, which have associated many more signals). Additional increase in the file size is the inclusion of Doppler observables (in the case of EBRE sometime between 2016 and 2018)."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#conclusions",
    "href": "posts/gnss_data_data_volume/index.html#conclusions",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the analyzed data, several key observations emerge:\n\nData Volume: For the CDDIS network, the daily data volume currently hovers around 2.5 GB, translating to approximately 1 TB/year. This figure excludes high-rate datasets recorded at 1-second intervals, which significantly increase the overall volume (at least by a factor of 30).\nStation Count: The number of stations within the CDDIS network has remained relatively stable at around 500 since 2014, which discards this parameter as the main cause for the volume increase.\nFile size: is the actual responsible of the overall folder size increase (rather than the station count). A notable surge in file and folder sizes started occuring in 2016, primarily due to the introduction of new constellations, associated signals, and the inclusion of additional observables such as e.g.¬†Doppler observables.\n\nThe substantial increase in data volume poses challenges beyond mere storage considerations. The RINEX format, while human-readable, is not optimized for machine processing, potentially hindering applications that require the analysis of large datasets (Big data, AI/ML, ‚Ä¶). To address this limitation, alternative binary (machine-friendly) formats should be considered as a complementary solution to enhance processing efficiency: a preliminary analysis show that while loading (parsing) a RINEX format in Python may take several tens of seconds (for a 5h 1-second interval RINEX V3 file), the same file stored in Apache Parquet takes little more than a second to load."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#acknowledgements",
    "href": "posts/gnss_data_data_volume/index.html#acknowledgements",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to NASA (and CDDIS) to provide the data used to elaborate this blog post."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#use-of-ai",
    "href": "posts/gnss_data_data_volume/index.html#use-of-ai",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial Intelligence has been used to polish some text styling and correct some typos. Ideation and data processing has been done by the author."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#footnotes",
    "href": "posts/gnss_data_data_volume/index.html#footnotes",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe same folder may contain data for the same receiver with different formats (e.g.¬†RINEX v2 and RINEX v3), the script below makes sure only unique stations are computed.‚Ü©Ô∏é"
  }
]