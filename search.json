[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Catal√†GalegoEnglish\n\n\nEls que es volen adre√ßar a mi, normalment ho fan pel meu nom, Miquel. Alguns, m√©s formals, utilitzen el meu primer cognom, Garcia.\nSe‚Äôm coneix per ser un enginyer de sistemes de navegaci√≥ (GPS pels profans en el tema), programador, somiatruites, aprenent etern i sobretot friqui, molt friqui.\n\n\nAqueles que queren dirixirse a min normalmente o fan polo meu nome, Miquel. Alg√∫ns, m√°is formais, usan o meu primeiro apelido, Garcia.\nCo√±√©cenme por ser un enxe√±eiro de sistemas de navegaci√≥n (GPS para os profanos no tema), programador, so√±ador, aprendiz eterno e sobre todo friqui, moi friqui.\n\n\nThose who want to address me usually do so by my name, Miquel. Some, more formally, use my first surname, Garcia.\nI am known to be a navigation systems engineer (GPS for those unfamiliar with the subject), programmer, dreamer, eternal learner, and a geek.\n\n\n\nEm podeu trobar per aqui\n   \nSi voleu seguir el canal de creaci√≥ de jocs que tinc a Telegram o WhatsApp, aqu√≠ us deixo els links:"
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "",
    "text": "As it is known, machine learning techniques try to model a certain function \\(f(x)\\) (or \\(y\\)) based on certain training data, that consists on known \\(f(x)\\) (output) values for certain \\(x\\) (input) points.\nIn order to tackle this problem, one can make assumptions on the underlying function behind \\(f(x)\\), for example a linear function:\n\\[\nf(x) = a \\cdot x + b\n\\]\nor a quadratic function\n\\[\nf(x) = a\\cdot x^2 + b\\cdot x + c\n\\]\nor as complex as needed. The approximation in those cases is known as parametric regression because the parameters (e.g.¬†\\(a\\), \\(b\\), \\(c\\), ‚Ä¶ ) will be estimated so that the difference between the function to approximate (\\(f(x)\\)) and our model minimizes the root mean square error (RMSE).\nBut, what happens if we do not know the underlying function? In this case, one can rely on Gaussian Processes (GPs), which offer a way to model \\(f(x)\\) in a probabilistic, non-parametric manner, avoiding assumptions about its analytic form (see for instance Section 15.9 of Press (2007)).\nIn order to understand how GP work, it is important to think of \\(f(x)\\), not as a continuous, closed-form, analytical expression such as, for example:\n\\[\nf(x) = -x^3+5\\cdot x^2 - \\frac{x}{2} -1\n\\]\nbut rather, as a set of probabilistic relationships between the function values. These relationships are provided by the kernel, which quantifies how the values of \\(f(x)\\) for the different inputs (\\(x\\)) are related. The kernel (or set of covariances) is computed based on the training data set. As an example, for the function given above, a possible training dataset could be (see Figure¬†1):\n\n\nImporting necessary modules\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\ndef f(x: np.array) -&gt; np.array:\n  return -1 * np.power(x, 3) + np.power(x, 2) * 5 - x * 0.5 - 1\n\n\nx =  np.array([0.       ,1.67 , 3.33 ,  5.0  ])\ny = f(x)\n\n\n\nprint training dataset\nformatter= {'float_kind': lambda x: \"%7.2f\" % x}\nprint(np.array2string(x, precision=2, separator=',', formatter=formatter))\nprint(np.array2string(y, precision=2, separator=',', formatter=formatter))\n\n\n[   0.00,   1.67,   3.33,   5.00]\n[  -1.00,   7.45,  15.85,  -3.50]\n\n\nOnce the kernel has been computed using the training data set, one can obtain the value of \\(f(x)\\) at an arbitrary point within the domain specified by the training data. An important feature that the GP offer is the provision of not only the value of the \\(f(x)\\) but also its formal uncertainty.\n\n\nCode\nfig = plt.figure(figsize=(9, 5))\n\nplt.title(\"Example of f(x)\")\nax = fig.gca()\n\nx_continuous = np.linspace(0, 5, 100)\nax.plot(x_continuous, f(x_continuous), label=\"continuous\")\nax.set_xlabel('x')\nax.set_ylabel('f(x)')\n\n\nx_discrete = x\nax.plot(x_discrete, f(x_discrete), \"or\", label=\"training dataset\")\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Example f(x) function with training data set\n\n\n\n\n\nThis post tries to reproduce the pictorial introduction of Gaussian Processes shown in Figure 1.1 of Williams and Rasmussen (2006) and is organized as follows: the first section will illustrate how a kernel is computed, using the GP equations, and the subsequent section will use an external library to train a GP (not only computing the kernel but also optimizing any hyperparameters set for the model)."
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#background",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#background",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "",
    "text": "As it is known, machine learning techniques try to model a certain function \\(f(x)\\) (or \\(y\\)) based on certain training data, that consists on known \\(f(x)\\) (output) values for certain \\(x\\) (input) points.\nIn order to tackle this problem, one can make assumptions on the underlying function behind \\(f(x)\\), for example a linear function:\n\\[\nf(x) = a \\cdot x + b\n\\]\nor a quadratic function\n\\[\nf(x) = a\\cdot x^2 + b\\cdot x + c\n\\]\nor as complex as needed. The approximation in those cases is known as parametric regression because the parameters (e.g.¬†\\(a\\), \\(b\\), \\(c\\), ‚Ä¶ ) will be estimated so that the difference between the function to approximate (\\(f(x)\\)) and our model minimizes the root mean square error (RMSE).\nBut, what happens if we do not know the underlying function? In this case, one can rely on Gaussian Processes (GPs), which offer a way to model \\(f(x)\\) in a probabilistic, non-parametric manner, avoiding assumptions about its analytic form (see for instance Section 15.9 of Press (2007)).\nIn order to understand how GP work, it is important to think of \\(f(x)\\), not as a continuous, closed-form, analytical expression such as, for example:\n\\[\nf(x) = -x^3+5\\cdot x^2 - \\frac{x}{2} -1\n\\]\nbut rather, as a set of probabilistic relationships between the function values. These relationships are provided by the kernel, which quantifies how the values of \\(f(x)\\) for the different inputs (\\(x\\)) are related. The kernel (or set of covariances) is computed based on the training data set. As an example, for the function given above, a possible training dataset could be (see Figure¬†1):\n\n\nImporting necessary modules\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\ndef f(x: np.array) -&gt; np.array:\n  return -1 * np.power(x, 3) + np.power(x, 2) * 5 - x * 0.5 - 1\n\n\nx =  np.array([0.       ,1.67 , 3.33 ,  5.0  ])\ny = f(x)\n\n\n\nprint training dataset\nformatter= {'float_kind': lambda x: \"%7.2f\" % x}\nprint(np.array2string(x, precision=2, separator=',', formatter=formatter))\nprint(np.array2string(y, precision=2, separator=',', formatter=formatter))\n\n\n[   0.00,   1.67,   3.33,   5.00]\n[  -1.00,   7.45,  15.85,  -3.50]\n\n\nOnce the kernel has been computed using the training data set, one can obtain the value of \\(f(x)\\) at an arbitrary point within the domain specified by the training data. An important feature that the GP offer is the provision of not only the value of the \\(f(x)\\) but also its formal uncertainty.\n\n\nCode\nfig = plt.figure(figsize=(9, 5))\n\nplt.title(\"Example of f(x)\")\nax = fig.gca()\n\nx_continuous = np.linspace(0, 5, 100)\nax.plot(x_continuous, f(x_continuous), label=\"continuous\")\nax.set_xlabel('x')\nax.set_ylabel('f(x)')\n\n\nx_discrete = x\nax.plot(x_discrete, f(x_discrete), \"or\", label=\"training dataset\")\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†1: Example f(x) function with training data set\n\n\n\n\n\nThis post tries to reproduce the pictorial introduction of Gaussian Processes shown in Figure 1.1 of Williams and Rasmussen (2006) and is organized as follows: the first section will illustrate how a kernel is computed, using the GP equations, and the subsequent section will use an external library to train a GP (not only computing the kernel but also optimizing any hyperparameters set for the model)."
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#drawing-sample-functions",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#drawing-sample-functions",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "Drawing sample functions",
    "text": "Drawing sample functions\nBefore showing how the kernel is computed, let‚Äôs first get an understanding on the meaning of ‚Äúdrawing a number of sample functions at random from the prior distribution‚Äù.\nLet‚Äôs assume that the output function is actually an array (not a continuous function) of \\(N\\) points, i.e.\n\\[\nf(x) = \\{y_1, y_2, y_3, ...y_N\\} \\quad for \\quad x = \\{x_1, x_2, x_3, ... x_N\\}\n\\]\nThen we assume that the array \\(f(x)\\) can be modelled as a set of random variables with a Gaussian distribution (i.e.¬†multivariate Gaussian distribution), where the individual data points are not independent (i.e.¬†covariance is not 0). If the points would be independent, this would be equivalent of having \\(N\\) univariate Gaussian random variables (one per each point), as shown in Figure¬†2. In this case, each function is in fact \\(N\\) realizations (one per each point) of a Gaussian distribution, and each point is independent to the previous.\n\n# Define input points (e.g., 100 evenly spaced points in 1D)\nX = np.linspace(-5, 5, 100)[:, None]\n\n# Zero mean function for all points (common GP assumption)\nmean = np.zeros(X.shape[0])\n\n\n\nCode\nK = np.diag([2.0] * len(mean))\n\n# Draw 5 random function samples from the GP prior (multivariate normal)\nsamples = np.random.multivariate_normal(mean, K, 5)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X, samples.T, lw=1.5)\nplt.title('Sample Functions Drawn from GP Prior (Zero Mean, Zero covariance kernel)')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†2: Set of functions drawn from a a multivariate Gaussian distribution where each data point is independent from the others\n\n\n\n\n\nIn order to not assume independency betweeen the points (and thus control the smoothness of the function), a kernel (\\(K\\)) needs to be specified. \\(K\\) is in fact a matrix containing the covariance of the different data points. For independent data points, the Kernel (\\(K\\)) would be diagonal (zero covariance). Intuitively, you can interpret the covariance as the similarity between two points of the data set: a smooth function implies that neighbouring points are very similar. In order to compute this covariance (or similarity, \\(K({\\bf x_1}, {\\bf x_2})\\)) it is common to use Radial Basis Functions over two arrays (\\({\\bf x_1}\\) and \\({\\bf x_2}\\)):\n\\[\nK({\\bf x_1}, {\\bf x_2})= \\exp \\left( -\\frac{\\| {\\bf x_1} - {\\bf x_2}\\|^2}{2 \\sigma^2}\\right)\n\\]\nThe Kernel function is defined in the rbf_kernel function below. Note that \\(\\sigma\\) controls the smoothnes of the function, the larger the \\(\\sigma\\), the smoother the functions will be, see Figure¬†3.\n\ndef rbf_kernel(x1, x2, sigma=1.0):\n    sqdist = np.sum(np.power(x1, 2), 1).reshape(-1, 1) + np.sum(np.power(x2, 2), 1) - 2 * np.dot(x1, np.transpose(x2))\n    return np.exp(-0.5 / sigma**2 * sqdist)\n\n\n\nPlotting samples functions for different sigmas\nsigma_small = 1.0\nK_small = rbf_kernel(X, X, sigma=sigma_small)\n\nsigma_large = 10.0\nK_large = rbf_kernel(X, X, sigma=sigma_large)\n\n# Draw 5 random function samples from the GP prior (multivariate normal)\nsamples_small = np.random.multivariate_normal(mean, K_small, 5)\nsamples_large = np.random.multivariate_normal(mean, K_large, 5)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X, samples_small.T, lw=1.5)\nplt.title(f'Sample Functions Drawn from GP Prior (Zero Mean, RBF Kernel, sigma={sigma_small})')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()\n\n# Draw 5 random function samples from the GP prior (multivariate normal)\nsamples = np.random.multivariate_normal(mean, K, 5)\n\nplt.figure(figsize=(10, 6))\nplt.plot(X, samples_large.T, lw=1.5)\nplt.title(f'Sample Functions Drawn from GP Prior (Zero Mean, RBF Kernel, sigma={sigma_large})')\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Small sigma (sigma=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Large sigma (sigma=10)\n\n\n\n\n\n\n\nFigure¬†3: Impact of sigma in Kernel functions"
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#prediction-without-training",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#prediction-without-training",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "Prediction without training",
    "text": "Prediction without training\nThis sections covers the fundamentals of establishing the predictive/posterior mean and variance using a fixed \\(\\sigma\\) (which is in fact a hyperparameter that will be optimized in a training step shown in the following session). Both the predictive/posterior mean and variance can be obtained using the closed-form analytical expression.\nLet‚Äôs use an observation dataset as follows\n\nX_train = [[-2], [0], [1], [2]]  # List of features\nY_train = [0.5, 1.2, 0.9, 3.3]\n\nWhy 2D dimension in X_train? Because X can be considered as an array of features and the distance function (the kernel) computes the distance between arrays of features.\nOnce the training dataset is defined, the predictive/posterior mean (\\(\\mu_*\\)) as well as the associated predictive/posterior variance (\\(\\sigma_*^2\\)) for a input to be predicted (\\(x_*\\)) can be computed using the following closed forms\n\\[\n\\begin{eqnarray}\n\\mu_* & = & m(x_*) + k_*^T \\cdot \\hat K \\cdot (y-m(X)) \\\\\n\\sigma_* & = & k(x_*, x_*)-k_*^T \\cdot \\hat K \\cdot k_* \\\\\n\\end{eqnarray}\n\\]\nwhere\n\\[\n\\hat K =  [K + \\sigma_n^2I]^{-1}\n\\]\nNote that, to compute the Kernel (\\(K\\)), two hyperparameters are required:\n\n\\(\\sigma\\), the smoothing factor discussed earlier\n\\(\\sigma_n\\), the noise variance, that can be interpreted as the observation noise (controls how much noise it is attributed to the observations)\n\n\n# Hyperparameters\nsigma = 1.0\nsigma_n = 1e-4\n\nIn this toy example these hyperparameters will be kept fixed, but in a machine learning context, they can be refined using an optimization algorithm such as gradient descent that will lead a set of hyperparameters that minimize the square root mean error.\nThe different elements in the previous equation are defined and computed as follows\n\n\\(\\hat K\\) is computed using \\(K\\), the covariance matrix using the observed dataset, as follows:\n\n\n# Compute kernel matrix for training points ($\\hat K$)\nK_hat = rbf_kernel(X_train, X_train, sigma)\nK_hat += sigma_n * np.eye(len(X_train))\nK_hat = np.linalg.inv(K_hat)\n\n\n\\(x_*\\) is the array with the test inputs:\n\n\n# Prepare Prediction Grid (Test Points)\nX_test = np.linspace(-3, 3, 100).reshape(-1, 1)  # Predict at 100 evenly spaced points\n\n\n\\(k_*\\) (K_s) is the vector of covariances between the new test points (\\(x_*\\)) and the observed input (\\(x\\)), and is computed as follows:\n\n\nK_s = rbf_kernel(X_train, X_test, sigma)\n\n\n\\(k(x_*, x_*)\\) (K_ss) the covariance of the test input (\\(x_*\\)) with itself\n\n\nK_ss = rbf_kernel(X_test, X_test, sigma)\n\n\n\\(m(x_*)\\) is the mean function at the test points (\\(x_*\\)), which is commonly 0 as stated in the problem definition\n\\(m(X)\\), the prior mean at the training input, is also commonly considered 0\n\nTherefore, the predictive/posterior mean at the test points (\\(\\mu_*\\)) can be computed as:\n\nmu_s = K_s.T @ K_hat @ Y_train\n\nAnd the predictive/posterior covariance at the test points (\\(\\sigma_*^2\\)), which will give an indication of the confidence of the prediction, will be computed as:\n\ncov_s = K_ss - K_s.T @ K_hat @ K_s  # Shape: (100,100)\nstd_s = np.sqrt(np.diag(cov_s))     # Predictive standard deviation (sigma)\n\nThe result of the prediction (for the fixed set of hyperparameters) can be then plotted (see Figure¬†4), showing that the predicted mean crosses exactly the training points.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(9, 5))\nplt.plot(np.array(X_train).flatten(), Y_train, \"ko\", label=\"Train data\")\nplt.plot(np.array(X_test).flatten(), mu_s, \"b\", lw=2, label=\"Posterior mean\")\nplt.fill_between(\n    X_test.flatten(),\n    mu_s - 2*std_s, mu_s + 2*std_s,\n    color=\"blue\", alpha=0.2, label=\"Uncertainty (2$\\sigma$)\"\n)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Gaussian Process Regression (Manual Training)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†4: GP regression with fixed hyperparameters"
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#prediction-with-training",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#prediction-with-training",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "Prediction with training",
    "text": "Prediction with training\nThis section will cover the usage of sklearn package in order to train the Gaussian Process and obtain a refined set of hyperparameters that minimises the root mean squared error (RMSE).\nAs the kernel function, we will use sklearn‚Äôs RBF class, which actually implements the rbf_kernel function shown above. Note that in the GaussianProcessRegressor class, it is possible to specify the alpha parameter, which can be interpreted as the variance of a Gaussian noise (\\(\\sigma_n\\)).\n\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF\n\nkernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(0.01, 100))\ngaussian_process = GaussianProcessRegressor(kernel=kernel, alpha=sigma_n**2, n_restarts_optimizer=9)\ngaussian_process.fit(X_train, Y_train)\ngaussian_process.kernel_\n\n1.79**2 * RBF(length_scale=0.664)\n\n\n\nmu_s_hyper, std_s_hyper = gaussian_process.predict(X_test, return_std=True)\n\nIn Figure¬†5, the result of this training is being shown (in addition, the result using manual training is also provided for reference). Note that in the positions of the training data set, there is also a certain uncertainty level that matches the \\(\\sigma_n\\) value specified before. If this value is increased, the uncertainty level of the prediction will increase accordingly.\n\n\nCode\nplt.figure(figsize=(9, 5))\nplt.plot(np.array(X_train).flatten(), Y_train, \"ko\", label=\"Train data\")\nplt.plot(np.array(X_test).flatten(), mu_s, \"b\", lw=2, label=\"Posterior mean\")\nplt.fill_between(\n    X_test.flatten(),\n    mu_s - 2*std_s, mu_s + 2*std_s,\n    color=\"blue\", alpha=0.2, label=\"Uncertainty (2$\\sigma$)\"\n)\nplt.plot(np.array(X_test).flatten(), mu_s_hyper, \"r\", lw=2, label=\"Posterior mean (with training)\")\nplt.fill_between(\n    X_test.flatten(),\n    mu_s_hyper - 2*std_s_hyper, mu_s_hyper + 2*std_s_hyper,\n    color=\"red\", alpha=0.2, label=\"Uncertainty (2$\\sigma$, with training)\"\n)\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Gaussian Process Regression (with Training)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†5: GP regression with trained hyperparameters"
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#conclusions",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#conclusions",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "Conclusions",
    "text": "Conclusions\nThis post attempted to provide with a very basic introduction on the fundamental concepts of the GP so that the reader may be able to build upon more complex scenarios and datasets, and also exploring potential limitations of the technique. One basic drawback of the GP processes is in fact that large training data sets may become impractical due to the fact that computing the Kernel function involves inverting a potentially large matrix (that grows with the size of the input data)."
  },
  {
    "objectID": "posts/2025-07-31_gaussian_processes_primer/index.html#usage-of-ai-in-this-post",
    "href": "posts/2025-07-31_gaussian_processes_primer/index.html#usage-of-ai-in-this-post",
    "title": "Gaussian Process Regression: A Beginner‚Äôs Primer",
    "section": "Usage of AI in this post",
    "text": "Usage of AI in this post\nIA tools (Perplexity) has been used to polish grammar and correct typos in the text. In addition, several code snippets have been created with AI as a starting point for implementation. Review and code refinements have been made by the author."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html",
    "title": "Com fer el teu Cronocartes",
    "section": "",
    "text": "El joc Cronocartes Hist√≤ria de Catalunya √©s un joc de cartes r√†pid i familiar en qu√® els jugadors competeixen ordenant una l√≠nia cronol√≤gica. Est√† basat en els populars jocs de cartes Timeline o Cardline.\nEl joc √©s f√†cilment portable a altres temes que requereixin una cronologia (moments √®pics de la saga Star Wars, hist√≤ria del barri de Sants de Barcelona‚Ä¶). De fet, diverses persones interessades en el format m‚Äôhan preguntat per consells de com fer una versi√≥ del Cronocartes, aix√≠ que us deixo aquest post amb alguns apunts i consells sobre el disseny per si us poden ser √∫tils."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#comen√ßa-la-casa-pel-terrat",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#comen√ßa-la-casa-pel-terrat",
    "title": "Com fer el teu Cronocartes",
    "section": "Comen√ßa la casa pel terrat",
    "text": "Comen√ßa la casa pel terrat\nNo s√≥c dissenyador gr√†fic, i una de les pors m√©s grans que tenia era fer un disseny que despr√©s hagu√©s d‚Äôanar modificant en funci√≥ del servei d‚Äôimpremta. Aix√≠ que, un cop decidit a fer la versi√≥ catalana del Cronocartes, vaig buscar un servei d‚Äôimpremta que tingu√©s aquestes caracter√≠stiques:\n\nPoder tenir un pressupost immediat via web, sense haver d‚Äôenviar formularis web, i fer-me una idea r√†pida de quan pujaria el cost m√©s important de la producci√≥.\nQue oferissin la mateixa mida de cartes que el Timeline per tal de poder-les combinar (Mini USA 41 x 63 mm)\nDisponibilitat de desc√†rrega de la plantilla per fer les cartes.\nFormat de la plantilla suportat per algun programari lliure (mireu la secci√≥ Eines avall).\n\nLa que vaig acabar escollint era Ludotipia1, i amb aquesta decisi√≥ va quedar fixat:\n\nEl n√∫mero total de cartes: 108 (l‚Äôaplicatiu web d√≥na diverses opcions, i aquesta era la m√©s propera a jocs similars)\nLa plantilla per maquetar les cartes, amb tots els marges necessaris. Teniu una imatge de la plantilla abaix tot i que us recomano que treballeu amb el PDF original per que no tingueu sorpreses amb les mides quan genereu els fitxers finals.\n\n\n\n\nPlantilla Tipia"
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#eines",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#eines",
    "title": "Com fer el teu Cronocartes",
    "section": "Eines",
    "text": "Eines\nEls programes de disseny gr√†fic com Adobe Photoshop i similars acostumen a estar fora del pressupost dels dissenyadors aficionats, pel que la opci√≥ de programari lliure √©s la opci√≥ m√©s econ√≤mica. De fet, s√≥n eines que cobreixen amb escreix les tasques que s‚Äôhan de realitzar. Us passo a continuaci√≥ una relaci√≥ de les eines que he utilitzat i l‚Äô√∫s que li he donat:\n\nScribus √©s una aplicatiu d‚Äôescriptori per autoedici√≥ que he utilitzat per maquetar les cartes. Una de les caracter√≠stiques fonamentals d‚Äôaquesta aplicaci√≥ √©s que permet fer el que es coneix com a ‚Äúmail-merge‚Äù: generar totes les cartes d‚Äôuna manera autom√†tica, r√†pida i senzilla a trav√©s d‚Äôuna sola plantilla i un fitxer de text en format Comma Separated Value (CSV) que cont√© totes les dades de cadascuna de les cartes.\nLibreOffice Calc per editar fulles de c√†lcul i guardar-les en format CSV. La idea √©s generar un fitxer en que cada fila contingui t√≠tol, any i imatge per cada carta. Amb aquest fitxer i la plantilla de maquetaci√≥ es poden generar totes les cartes de la baralla (i no haver d‚Äôanar muntant les cartes una a una). Al ser fitxers de text, la veritat √©s que hi han moltes alternatives per editar CSV: si ets programador, pots utilitzar VS Code amb l‚Äôextensi√≥ Edit CSV.\nGimp, programari lliure d‚Äôedici√≥ gr√†fica, indispensable per manipular les imatges (sobretot escalat, rotacions, crop, manipulaci√≥ de colors i filtrat d‚Äôimatges, ‚Ä¶)\n\nEl sistema operatiu que utilitzo √©s Ubuntu/Linux, per√≤ tot aquest programari t√© versions per altres sistemes operatius."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#events",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#events",
    "title": "Com fer el teu Cronocartes",
    "section": "Events",
    "text": "Events\nJuntament amb la creaci√≥ de les Imatges, el llistat d‚Äôevents √©s el m√©s complicat. Encara que no ho sembli, buscar 100 events hist√≤rics que estiguin documentats i m√©s o menys distribu√Øts en els 2000 anys d‚Äôhist√≤ria pot ser complicat. En el cas del Cronocartes Hist√≤ria de Catalunya vaig tenir en compte els seg√ºents punts:\n\nVolia afegir un punt de complexitat aix√≠ que la majoria de cartes estan concentrades a finals del segle XIX i principis del XX.\nTrobar events del primer mil¬∑leni pot ser un repte per que no hi han gaires refer√®ncies escrites (a excepci√≥ d‚Äôevents clau com guerres, tractats o revoltes).\nHi ha vegades √©n que √©s impossible saber l‚Äôany exacte d‚Äôun event. En aquests casos podeu posar una c. (del llat√≠ circa, aproximadament) abans de l‚Äôany. Una altra alternativa m√©s f√†cil d‚Äôinterpretar pot ser ~.\nSi us costa trobar la data d‚Äôalgun event, opteu per posar la primera refer√®ncia escrita sobre l‚Äôevent (el cas paradigm√†tic del Cronocartes Hist√≤ria de Catalunya √©s el Mat√≥, sabeu de quin any es t√© la primera refer√®ncia? üòú)\nL‚Äôespai pel text de l‚Äôevent √©s redu√Øt (m√©s del que sembla). Haureu de ser creatius per reduir el nombre de car√†cters! No dubteu a utilitzar ordinals en comptes de paraules (per exemple 1r en comptes de Primer). Algunes vegades he hagut de rec√≥rrer a un subt√≠tol (amb un tipus de lletra m√©s petit) en cas que calgu√©s donar una mica m√©s de context, com es mostra en l‚Äôexemple que segueix:\n\n\n\n\nPont del diable, exemple de subt√≠tol"
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#imatges",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#imatges",
    "title": "Com fer el teu Cronocartes",
    "section": "Imatges",
    "text": "Imatges\nPer les imatges de la versi√≥ d‚ÄôHist√≤ria de Catalunya vaig rec√≥rrer a imatges d‚Äôinternet i alguna composici√≥ pr√≤pia. Compteu moltes hores de manipulaci√≥ d‚Äôimatges amb GIMP per acabar d‚Äôajustar-les al format de les cartes. Cal dir que tamb√© √©s una de les parts m√©s creatives del proc√©s, i les possibilitats i eines que dona un programa com GIMP s√≥n innumerables.\nArribats a aquest punt, haureu de vigilar amb els drets d‚Äôautor, especialment si voleu distribuir comercialment el producte. Assegureu-vos que les imatges es poden utilitzar per finalitats comercials. Si teniu dubtes, considereu altres alternatives. En el meu cas, per altres versions estic utilitzant eines d‚Äôintel¬∑lig√®ncia artificial que a m√©s agilitzen substancialment el proc√®s de creaci√≥ d‚Äôimatges a partir d‚Äôun prompt o una altra imatge (o combinaci√≥ d‚Äôimatges). Les eines que he utilitzat s√≥n:\n\nKrea.ai, eina bastant potent que permet generar imatges a partir d‚Äôuna frase o b√© a partir d‚Äôaltres imatges.\nUncrop, eina que permet estendre una imatge (uncrop) i arribar a una certa resoluci√≥ o mida. Molt m√©s √∫til del que pot semblar a simple vista, sobretot per fotografies antigues amb mida redu√Øda. A baix incloc un exemple on la imatge original (esquerra) s‚Äôha modificat per estendre-la per la part superior i generar un sostre (imatge de la dreta).\n\n\n\n\n\n\n\n\nImatge original\nImatge uncropped\n\n\n\n\n\n\n\n\n\nAmbdues eines s√≥n gratu√Øtes, per√≤ tenen funcionalitat limitada. En el meu cas, els l√≠mits d‚Äô√∫s s√≥n m√©s que suficients, per√≤ m‚Äôhe plantejat m√©s d‚Äôun cop concentrar la feina de modificaci√≥ d‚Äôimatges en un mes i subscriure‚Äôm a aquests serveis un parell de mesos per tenir acc√©s a m√©s prestacions que poden ser √∫tils (processat m√©s r√†pid i cap limitaci√≥ en el nombre d‚Äôimatges generades, eliminaci√≥ del fons, ‚Ä¶)."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#automatitzant-el-proc√©s",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#automatitzant-el-proc√©s",
    "title": "Com fer el teu Cronocartes",
    "section": "Automatitzant el proc√©s",
    "text": "Automatitzant el proc√©s\nFer un joc de taula √©s un proc√©s iteratiu, pel que cal tenir un m√®tode que permeti:\n\nRegenerar totes les cartes d‚Äôuna manera r√†pida i √†gil per tal de poder provar f√†cilment v√†ries opcions de disseny i maquetaci√≥.\nTenir un control de les diferents versions que es van fent del mateix. D‚Äôaquesta manera tot el proc√©s de creaci√≥ ser√† reproduible i tra√ßable.\n\nAutomatitzar tot el proc√©s garanteix la consistencia en el disseny de tot el joc i evita els ‚Äúcasos especials‚Äù que acaben sent un malson quan es va modificant el disseny. Sempre que podeu, intenteu pensar com automatitzar el disseny des del principi de la creaci√≥ del joc.\nPer aquest motiu l‚Äôelecci√≥ de les eines √©s critica. Una de les prestacions m√©s important d‚ÄôScribus √©s el fet de poder fer mail-merge: creaci√≥ de totes les cartes a partir d‚Äôuna sola plantilla i un CSV amb les dades de totes les cartes. Aquest proc√©s es pot realitzar amb el plug-in Scribus Generator."
  },
  {
    "objectID": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#footnotes",
    "href": "posts/com-fer-el-teu-cronocartes/Com_fer_el_teu_Cronocartes.html#footnotes",
    "title": "Com fer el teu Cronocartes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNo tinc cap vincle contractual amb ells ni cobro cap mena de comissi√≥ o ingr√©s de publicitat per part seva.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/hatanaka_lib/index.html",
    "href": "posts/hatanaka_lib/index.html",
    "title": "Native integration of Hatanaka for Python",
    "section": "",
    "text": "Data servers hosting GNSS data in RINEX format typically employ Hatanaka-gzip compression to optimize storage. This is becoming critical as networks and number of GNSS signals grow. While previous efforts have explored storing data in binary formats like Parquet, these formats cannot beat the compression achieved by the highly specialized Hatanaka + gzip combination (crx.gz files).\nHowever, users of this data have to make sure that the hatanaka software is installed in their system. This requirement stems from the need for pre-processing, specifically converting the compressed files (crx.gz) to text RINEX files. This pre-processing typically involves the following steps::\nIn programming languages like Python, these steps typically involve calling external tools using the subprocess library, essentially executing external programs. These programs often store their output in intermediate files, which are then consumed by subsequent processing steps. This multiple writing and parsing (text) files (first Hatanaka file and then uncompress RINEX format) to load it into memory for processinng significantly increases execution time.\nTo leverage existing GNSS data stored in crx.gz format while simplifying pre-processing (reducing dependencies) and accelerating loading times (by eliminating multiple file writes/reads), a refactoring of the Hatanaka software is proposed. This refactoring would facilitate the development of a C extension for Python, optimizing the entire pre-processing pipeline.\nThe main goal of this work is to deploy a software that:\nSomething on those lines:\n# The code makes use of the external roktools library, installed with the pip\n# command\nfrom roktools import hatanaka\n\n# Use native integration using C-extension to directly load the data into a\n# Dataframe. The function does not rely on calling external programs.\ndf = hatanaka.to_dataframe(\"ACSO00XXX_R_20241310000_05S_01S_MO.crx.gz\")"
  },
  {
    "objectID": "posts/hatanaka_lib/index.html#refactoring-the-hatanaka-library",
    "href": "posts/hatanaka_lib/index.html#refactoring-the-hatanaka-library",
    "title": "Native integration of Hatanaka for Python",
    "section": "Refactoring the Hatanaka library",
    "text": "Refactoring the Hatanaka library\nThe Hatanaka library developed by Yuri Hatanaka is fundamentally two source files: the two main programs to compress and decompress RINEX files. Also, scripts that handle the gzip (binary) compression are also provided in the software.\nTo facilitate native integration with other programming languages (via C-extensions for Python or Rust), the core processing logic must be extracted from these main source files. Specifically, the refactoring focuses on the crx2rnx.c source file, as the primary interest lines in the uncompression stage. This involves isolating the core processing logic (the controller) from input/output operations based on fprintf and fgets methods (the ‚Äúview‚Äù). This separation allows for code reuse with various application layers, such as Python scripts (or even other languages like Rust that support C-extensions), beyond the original C-executables (crx2rnx).\nA key benefit of this approach is the potential to eliminate the need to write the uncompressed RINEX file to disk and then parse it again to load it into memory. Instead, the data can be extracted directly from the Hatanaka-compressed file and loaded into memory, thereby avoiding unnecessary file writes and reads. It‚Äôs important to note that binary uncompression will still be required, but Python provides built-in libraries to handle this efficiently.\n\n\n\n\n\n\nWarningExperimental feature\n\n\n\nThe hatanaka.to_dataframe feature is currently unavailable on roktools due to platform-dependent considerations. Deployment mechanisms are still under investigation.\nFor inquiries, create an issue to Github or visit GNSS R&D channel in X."
  },
  {
    "objectID": "posts/hatanaka_lib/index.html#performance",
    "href": "posts/hatanaka_lib/index.html#performance",
    "title": "Native integration of Hatanaka for Python",
    "section": "Performance",
    "text": "Performance\nTo assess the performance gains of the proposed approach, loading times using various strategies on files of different sizes have been measured.\n\nTest data\nThe data used for this analysis cover:\n\nAround 500 daily and hourly Hatanaka-gzip compressed files (crx.gz) for January 1st, 2025, were obtained from the CDDIS server.\nDaily files contain samples every 30 seconds, while hourly files contain data every second (high rate). This ensures a variety of file sizes, ranging from 1 to 8 MB.\n\n\n\nLoading strategies\nThree approaches were evaluated for loading the files and measuring their associated loading times:\n\nDirect. This strategy utilizes the proposed refactoring for direct CRZ loading:\n\nUncompress gz file and extract the CRX (Hatanaka compressed). This uses the Python built-in gzip library\nDirect read of the CRX file into a pandas dataframe.\n\nSystem. This approach implements the standard processing pipeline mentioned earlier, involving two steps:\n\nUse system call to CRZ2RNX to obtain the RINEX file\nLoad the Rinex using the py-roktools rinex library\n\nPre-loaded Parquet: Assume that RINEX files have been pre-loaded and stored as parquet format, as proposed in a previous work. In this step, data is stored in binary format, therefore no parsing is needed: just a simple load of a parquet file (i.e.¬†using the method pandas.read_parquet).\n\n\n\nResults\nThe following figure shows how the loading times compare between these methods. As expected, the direct method offers a much faster performance (a consistent 25% of improvement relative to the system approach) accross all file sizes.\nNote that the parquet strategy cannot be directly compared to the direct and system strategies as it is an unfair comparison (i.e.¬†no parsing of text file is performed). However it is provided for completeness, to illustrate the potential benefits of using a binary format to store the data.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_direct = pd.read_parquet('direct_crxgz.parquet')\ndf_system = pd.read_parquet('system_call.parquet')\ndf_parquet = pd.read_parquet('parquet_load.parquet')\n\non = ['file', 'interval_s', 'filesize']\ndf = pd.merge(df_direct, df_system, how = 'inner', on = on, suffixes=('_direct', '_system'))\ndf = pd.merge(df, df_parquet, how = 'inner', on = on, suffixes=('', '_parquet'))\ndf['fs'] = (df['filesize'] / 1e6).round(0)\ndf_stats = df.groupby('fs')[['load_time_s_direct', 'load_time_s_system', 'load_time_s']].median().reset_index()\n\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nax.set_title('Loading times of GNSS Hatanaka compressed files')\n\nax.plot(df_direct['filesize'] / 1.0e6, df_direct['load_time_s'], '.r', alpha=0.25, label=\"\")\nax.plot(df_system['filesize'] / 1.0e6, df_system['load_time_s'], '.g', alpha=0.25, label=\"\")\nax.plot(df_parquet['filesize'] / 1.0e6, df_parquet['load_time_s'], '.b', alpha=0.25, label=\"\")\nax.plot(df_stats['fs'], df_stats['load_time_s_direct'], '-r', label='Direct')\nax.plot(df_stats['fs'], df_stats['load_time_s_system'], '-g', label='System')\nax.plot(df_stats['fs'], df_stats['load_time_s'], '-b', label='Parquet')\n\nax.set_ylim(0,15)\nax.set_xlabel('File size [MB]')\nax.set_ylabel('Loading time [s]')\nax.legend(loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nLoading times of GNSS Hatanaka compressed files\n\n\n\n\nFor a more quantitative evaluation, the loading times (in seconds) for each of the strategies and for the various file sizes are collated in the following table\n\n\nCode\nfrom IPython.display import Markdown\n\ndf2 = df_stats.round(2)\ndf2 = df2.rename(columns={'fs': 'File Size [MB]', 'load_time_s_direct': 'Direct', 'load_time_s_system': 'System', 'load_time_s': 'Parquet'})\nMarkdown(df2.to_markdown(headers='keys', numalign=\"center\", index=False))\n\n\n\n\n\nFile Size [MB]\nDirect\nSystem\nParquet\n\n\n\n\n0\n0.74\n1.51\n0.01\n\n\n1\n1.92\n2.72\n0.02\n\n\n2\n3.11\n3.93\n0.03\n\n\n3\n3.79\n5.68\n0.04\n\n\n4\n7.07\n8.26\n0.06\n\n\n5\n7.72\n9.14\n0.06\n\n\n6\n8.58\n9.87\n0.09\n\n\n7\n8.6\n11.12\n0.11\n\n\n8\n9.54\n13.07\n0.16"
  },
  {
    "objectID": "posts/hatanaka_lib/index.html#conclusions",
    "href": "posts/hatanaka_lib/index.html#conclusions",
    "title": "Native integration of Hatanaka for Python",
    "section": "Conclusions",
    "text": "Conclusions\nThis post introduces a pre-processing pipeline for Hatanaka + gzip compressed files based on native integration directly with scripting languages like Python. This approach eliminates the need for external tools, significantly reducing processing time. This improvement is particularly beneficial for applications that handle large volumes of GNSS data, such as precise orbit and clock calculations.\nResults obtained with the process of almost 500 files with different size show a consistent speedup when using this native implementation by about 25 % relative to relying on external system calls. The loading speed can be even dramatically increase when using binary format such as parquet, that offer a tenfold improvement in the loading time.\nWhile this approach offers significant advantages, there is still room for optimization. The current implementation leverages the original Hatanaka library, which generates intermediate strings that are subsequently parsed into floating-point numbers using the C atof method. Future improvements could involve eliminating these intermediate strings and directly computing the floating-point values from the CRX file."
  },
  {
    "objectID": "posts/hatanaka_lib/index.html#acknowledgements",
    "href": "posts/hatanaka_lib/index.html#acknowledgements",
    "title": "Native integration of Hatanaka for Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe author would like to extend his gratitude to the Crustal Deformation Data Information System for the GNSS data used in this work."
  },
  {
    "objectID": "posts/hatanaka_lib/index.html#use-of-ai",
    "href": "posts/hatanaka_lib/index.html#use-of-ai",
    "title": "Native integration of Hatanaka for Python",
    "section": "Use of AI",
    "text": "Use of AI\nThis text has been enhanced using Artificial Intelligence to refine its style and correct typos. The author was responsible for all stages of the process, including ideation, coding, data processing, initial text drafting, and review of the AI‚Äôs contributions."
  },
  {
    "objectID": "posts/particle_filter.html",
    "href": "posts/particle_filter.html",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "",
    "text": "This demo serves as demonstration on how to use the particle filter module shipped within this repository. The notebook simulates a rover moving in 2D with constant-velocity dynamics and range measurements to fixed beacons. We will:"
  },
  {
    "objectID": "posts/particle_filter.html#particle-filters-brief-introduction",
    "href": "posts/particle_filter.html#particle-filters-brief-introduction",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Particle filters ‚Äî brief introduction",
    "text": "Particle filters ‚Äî brief introduction\nParticle filters (also known as Sequential Monte Carlo methods) are a family of simulation-based algorithms that approximate the posterior distribution of a system‚Äôs state by propagating and resampling a set of weighted samples (particles), enabling state estimation for strongly nonlinear or non‚ÄëGaussian models.\n\nIntuition\nThink of a particle filter like a crowd of guesses about the state you want to estimate. In a positioning context, this state is usually the location where an object might be. Each guess (a particle) is a complete possible state (for example: x, y, vx, vy). As the system moves and we get measurements, we score each guess by how well it explains the measurements. We then keep the better guesses, copy them (more often if they are good), and add a bit of random jitter so the crowd explores nearby alternatives. Over time the crowd concentrates around the true state.\nWhy a Particle filter is helpful:\n\nWorks when measurements or motion are complicated or noisy ‚Äî the crowd can represent many possibilities.\nCan represent multiple plausible locations at once (not forced into a single average).\n\n\n\nKey differences vs EKF/UKF\n\nRepresentation: EKF/UKF approximate the posterior with a single Gaussian (mean + covariance). Particle Filters (PF) approximate the full posterior with many weighted samples and can model any arbitrary posterior distribution (if the particle sampling is good enough)\nLinearity / Gaussian assumptions: EKF linearizes the dynamics/measurements, while PF can handle highly non-linear models.\nComputational cost: EKF/UKF are generally cheaper (\\(O(n^3)\\) for covariance operations with state dimension n) whereas particle filters scale roughly \\(O(N_{particles} \\cdot cost_{particle})\\) and can become expensive as the number of particles or state dimension grows (though particle updates are easily parallelizable). This is the main drawback of PF against other filters such as e.g.¬†EKF/UKF.\nDegeneracy & resampling: Particle filters can suffer sample impoverishment (most of the weight concentrates on a few particles), so strategies must be put in place to focus effort on high‚Äëweight particles while maintaining diversity and preserving exploration.\n\n\n\nWhen to use particle filters\n\nNonlinear, non-Gaussian problems where the posterior is far from Gaussian (e.g., multi-modal)\nFor low-to-moderate state dimensions."
  },
  {
    "objectID": "posts/particle_filter.html#hands-on-particle-filter",
    "href": "posts/particle_filter.html#hands-on-particle-filter",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Hands-on Particle Filter",
    "text": "Hands-on Particle Filter\nIn this notebook you will find a tutorial on the basics aspects of the PF applied to a simple navigation problem and how these aspects are represented in the pygnss.filter.particle module.\n\nPrerequisites\nTo run this notebook, make sure you have the following packages included in your system\nmatplotlib\nnumpy\npygnss\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pygnss\n\nA Particle Filter (and in fact all estimation filters) require a Model that informs how to propagate the state in time and how to translate the state to the observations.\n\npropage_state: given a state at epoch \\(k\\), generate the prior state for the next epoch \\(k+1\\). In a navigation context, it would consist in propagating the position based on the knowledge of the previous position and velocity.\nto_observations: given a state, generate the observations. This is used to compute the residuals relative to the actual observations, that will later lead to the innovation that will be applied to the estimated state.\n\nA Model interface is provided in the pygnss.filter module. We are going to derive a constant-velocity 2D model with range observations to fixed beacons (i.e.¬†ConstantVelocityRange2D)\n\nfrom pygnss.filter import Model, ModelObs\n\nclass ConstantVelocityRange2D(Model):\n\n    def __init__(self, dt: float, beacons: np.ndarray):\n        self.dt = float(dt)\n        self.beacons = np.asarray(beacons, dtype=float)\n        self._Phi = np.array([[1, 0, dt, 0],\n                              [0, 1, 0, dt],\n                              [0, 0, 1,  0],\n                              [0, 0, 0,  1]], dtype=float)\n\n    def propagate_state(self, state: np.ndarray) -&gt; np.ndarray:\n        return self._Phi @ state\n\n    def to_observations(self, state: np.ndarray, compute_jacobian: bool = False, **kwargs) -&gt; ModelObs:\n        \"\"\"\n        Compute range measurements to beacons from the current state.\n        \"\"\"\n        pos = state[:2]\n        rho = pos - self.beacons\n        ranges = np.sqrt(np.sum(rho**2, axis=1))\n        H = None\n        if compute_jacobian:\n            dr_dpos = rho / ranges[:, np.newaxis]\n            H = np.zeros((len(ranges), 4))\n            H[:, :2] = dr_dpos\n        return ModelObs(ranges, H)\n\n    def Phi(self):\n        return self._Phi\n\nThe Filter class also requires a class to handle the estimated state (i.e. print to a file, make plots, ‚Ä¶)\n\nfrom pygnss.filter import StateHandler\n\nclass HistoryHandler(StateHandler):\n    def __init__(self):\n        self.states = []\n        self.postfits = []\n\n    def process_state(self, state: np.ndarray, covariance_matrix: np.ndarray, **kwargs):\n        self.states.append(np.array(state, dtype=float).copy())\n        if 'postfits' in kwargs and kwargs['postfits'] is not None:\n            self.postfits.append(np.array(kwargs['postfits'], dtype=float).copy())"
  },
  {
    "objectID": "posts/particle_filter.html#scenario",
    "href": "posts/particle_filter.html#scenario",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Scenario",
    "text": "Scenario\nTo demonstrate the application of the PF, we are going to simulate a 2D navigation case. The following code snippet simulates a trajectory as well as some noise ranges (i.e.¬†distances between the rover and each of the beacons). The ranges will be the measurements to be ingested by the PF to estimate the position\n\n# Simulate ground-truth motion and noisy range measurements\nnp.random.seed(42)\ndt = 1.0\nsteps = 150\n\n# Beacons in 2D space (meters)\nbeacons = np.array([[0, 0], [0, 50], [50, 0], [80, 80], [25, 25]], dtype=float)\n\n# True initial state [x, y, vx, vy]\nx0_true = np.array([5.0, 5.0, 0.8, 0.5])\n\n# Process (acceleration) and measurement noise\nsigma_a = 0.05   # m/s^2\nsigma_r = 0.8    # m range noise\n\n# Constant-velocity model matrices\nF = np.array([[1, 0, dt, 0],\n              [0, 1, 0, dt],\n              [0, 0, 1,  0],\n              [0, 0, 0,  1]], dtype=float)\nG = np.array([[0.5*dt**2, 0],\n              [0, 0.5*dt**2],\n              [dt, 0],\n              [0, dt]], dtype=float)\n\n# Simulate trajectory\nx_true = np.zeros((steps, 4))\nx_true[0] = x0_true\nfor k in range(1, steps):\n    a = sigma_a * np.random.randn(2)\n    x_true[k] = F @ x_true[k-1] + G @ a\n\ndef ranges_from_state(state):\n    pos = state[:2]\n    rho = pos - beacons\n    return np.sqrt(np.sum(rho**2, axis=1))\n\n# Generate noisy range measurements\nranges = np.zeros((steps, beacons.shape[0]))\nfor k in range(steps):\n    ranges[k] = ranges_from_state(x_true[k]) + sigma_r * np.random.randn(beacons.shape[0])\n\n# Visualize ground truth and beacons (use matching colors for ranges)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n# Get the default color cycle used by matplotlib so we can reuse the same colors\nprop_cycle = plt.rcParams.get('axes.prop_cycle').by_key()\ncolors = prop_cycle.get('color', list(plt.cm.tab10.colors))\n# If there are fewer colors than beacons, repeat the cycle to cover all beacons\nif len(colors) &lt; len(beacons):\n    colors = (colors * ((len(beacons) // len(colors)) + 1))[:len(beacons)]\n# Plot beacons using the same color assigned to each beacon's range series\nax1.scatter(beacons[:, 0], beacons[:, 1], c=colors[:len(beacons)], marker='o', s=100, label='Beacons')\nax1.plot(x_true[:, 0], x_true[:, 1], 'k-', label='True path')\nax1.axis('equal')\nax1.set_xlabel('X [m]')\nax1.set_ylabel('Y [m]')\nax1.grid(True)\nax1.legend()\nax1.set_title('Ground truth path and beacons')\n\n\n# Visualize ranges (plot per-beacon and reuse same colors)\nfor i in range(ranges.shape[1]):\n    ax2.plot(ranges[:, i], color=colors[i], label=f'Beacon {i}')\nax2.grid(True)\nax2.set_xlabel('Epoch [s]')\nax2.set_ylabel('Range [m]')\nax2.legend(loc='upper right', fontsize='small')\n_ = ax2.set_title('Ranges between rover and each beacon')\n\nplt.show()"
  },
  {
    "objectID": "posts/particle_filter.html#filter-setup",
    "href": "posts/particle_filter.html#filter-setup",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Filter setup",
    "text": "Filter setup\nIn order to setup the filter, we will be requiring the following:\n\nA class that computes the weight for each sample, we will be using WeightEstimatorGaussian which computes the weight of each particle based on the likelihood\n\n\nfrom pygnss.filter.particle import WeightEstimatorGaussian\n\n\nAn initial set of particles (candidates of possible positions). This will be refined as measurements are processed\n\n\n\n# Initialize particle filter\nN = 1000  # number of particles\nx0_guess = np.array([2.0, 2.0, 0.0, 0.0])\nP0_pos = 10.0  # meters std\nP0_vel = 1.0   # m/s std\n\nparticles = []\nfor _ in range(N):\n    s = x0_guess + np.array([\n        P0_pos*np.random.randn(),\n        P0_pos*np.random.randn(),\n        P0_vel*np.random.randn(),\n        P0_vel*np.random.randn(),\n    ])\n    particles.append(s)\n\n\n# Set up the particle filter components: the model, weight estimator, and state handler\nmodel = ConstantVelocityRange2D(dt=dt, beacons=beacons)\nweight_estimator = WeightEstimatorGaussian()\nhandler = HistoryHandler()\n\n# The roughening is a process to add some noise to the particles after resampling to maintain diversity\n# we are adding a noise with standard deviation sigma_r to each state dimension\n# (the sigma_r defined earlier for measurement noise is a reasonable choice here)\nroughening = [sigma_r, sigma_r, sigma_r, sigma_r]\n\nOnce the different components are defined, we are now in the position of instantiating the filter\n\nfrom pygnss.filter.particle import Filter\n\npf = Filter(\n    initial_states=particles,\n    weight_estimator=weight_estimator,\n    model=model,\n    state_handler=handler,\n    roughening_std=roughening,\n)\n\nR = (sigma_r**2) * np.eye(beacons.shape[0])\nprint('PF initialized with', len(particles), 'particles')\n\nPF initialized with 1000 particles\n\n\nFinally, the measurements can be processed sequentially\n\nfor k in range(steps):\n    yk = ranges[k]\n    pf.process(yk, R)\n\nUnder the hood, while the measurements are processed and estimates obtained, they are forwarded into the handler class defined above. You can access the final estimates with the following command:\n\nestimates = np.array(handler.states)\n\nAnd the plots of position and errors can be obtained as well\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 5))\naxs[0].plot(x_true[:, 0], x_true[:, 1], 'k-', label='True')\naxs[0].plot(estimates[:, 0], estimates[:, 1], 'b.', label='PF estimate')\naxs[0].scatter(beacons[:, 0], beacons[:, 1], c='red', marker='^', label='Beacons')\naxs[0].set_title('2D trajectory')\naxs[0].set_xlabel('X [m]')\naxs[0].set_ylabel('Y [m]')\naxs[0].axis('equal')\naxs[0].grid(True)\naxs[0].legend()\n\naxs[1].plot(estimates[:, 0] - x_true[:, 0], label='$\\Delta x$')\naxs[1].plot(estimates[:, 1] - x_true[:, 1], label='$\\Delta y$')\naxs[1].set_title('Position error')\naxs[1].set_xlabel('Time [s]')\naxs[1].set_ylabel('Error [m]')\naxs[1].grid(True)\naxs[1].legend()\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nAlso, some basic performance metrics can be obtained since we know the actual position\n\npos_err = np.linalg.norm(estimates[:, :2] - x_true[:, :2], axis=1)\nvel_err = np.linalg.norm(estimates[:, 2:] - x_true[:, 2:], axis=1)\nprint(f'Position RMSE: {np.sqrt(np.mean(pos_err**2)):.3f} m')\nprint(f'Velocity RMSE: {np.sqrt(np.mean(vel_err**2)):.3f} m/s')\n\nPosition RMSE: 1.922 m\nVelocity RMSE: 1.680 m/s\n\n\nAn additional quality metrics to evaluate the performance of the filter are the postfit residuals, which give information on the errors that could not be absorbed by the estimated parameters. In this simulation, these errors correspond to Additive White Gaussian Noise (AWGN), therefore, the postfit residuals should be distributed via a Gaussian function with zero mean and sigma_r standard deviation.\n\npostfits = np.array(handler.postfits).flatten()\n\nplt.figure(figsize=(5, 3))\nplt.hist(postfits, bins=30, alpha=0.7)\nplt.title('Postfit residuals')\nplt.grid(True)\n\nprint(f'Postfit Mean: {np.mean(postfits):.3f} m')\nprint(f'Postfit Std:  {np.std(postfits):.3f} m')\n\nPostfit Mean: 0.036 m\nPostfit Std:  1.373 m"
  },
  {
    "objectID": "posts/particle_filter.html#conclusions",
    "href": "posts/particle_filter.html#conclusions",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Conclusions",
    "text": "Conclusions\nThis notebook demonstrates a working particle filter for a 2D constant-velocity rover using range measurements to fixed beacons. The example is intentionally simple but highlights the main pieces required to run a particle filter: model propagation, an observation mapping, weight computation, resampling, and a state handler to record results.\nKey takeaways:\n\nThe particle filter converges from a relatively coarse prior and is able to track both position and velocity of the rover over time when supplied with per-epoch range measurements.\nPostfit residuals (stored in the handler) are useful to check consistency: in this setup they are approximately zero-mean and their spread is comparable to the measurement noise specified by sigma_r (see the histogram and printed statistics above).\n\nPractical notes and next steps:\n\nIncrease or decrease N (number of particles) to explore the accuracy vs computational cost trade-off.\nTry alternative resampling schemes, adjust the roughening_std, or use informed proposal distributions to mitigate sample impoverishment in long runs.\nExtend the model to include sensor biases, additional measurement types (e.g., bearings), or 3D motion for more realistic scenarios.\nAdd automated tests that exercise the filter on known trajectories to track regressions when changing the implementation.\n\nReproducibility:\n\nTo reproduce the figures and metrics in this notebook, run the cells top-to-bottom. Key parameters to try are N (particles), sigma_r (measurement noise), beacon positions (beacons), and the roughening_std used after resampling."
  },
  {
    "objectID": "posts/particle_filter.html#further-reading",
    "href": "posts/particle_filter.html#further-reading",
    "title": "Particle filter demo: 2D rover position and velocity",
    "section": "Further reading",
    "text": "Further reading\n\nElfring, Jos, Elena Torta, and Ren√© Van De Molengraft. ‚ÄúParticle filters: A hands-on tutorial.‚Äù Sensors 21, no. 2 (2021): 438."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html",
    "href": "posts/farewell-rokubun/rokubun.html",
    "title": "On my journey as CTO at Rokubun",
    "section": "",
    "text": "For the last ten years I‚Äôve been Chief Technological Officer (CTO) at Rokubun, a company I co-founded and co-managed with my colleague Xavier Banque. Unfortunately, we had to take the difficult decision of shutting it down.\nDrawing on my experience at Rokubun, I‚Äôd like to share some insights and some takeaways that could hopefully help other entrepreneurs at similar fields in which we operated."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#what-was-rokubun",
    "href": "posts/farewell-rokubun/rokubun.html#what-was-rokubun",
    "title": "On my journey as CTO at Rokubun",
    "section": "What was Rokubun?",
    "text": "What was Rokubun?\nRokubun aimed at developing navigation and positioning solutions1 with the goal of creating a product or service that could generate recursive sales. Technological innovation was an important piece in this plan, as an engine to increase the technological and scientific capital of the company ahd keep its portfolio competitve with the new developments in the navigation ecosystem.\nRokubun started as a consultancy business for satellite-based navigation technology, thus leveraging our past expertise in the field. In practice, this meant that we bid for tenders issued by the European Space Agency (ESA) and other funding vehicles. In fact, Rokubun was born as part of the ESA Business Incubation Programme that was established in Barcelona in 2015.\nWe sticked to this plan and along the way we created Jason, a GNSS Post Processing Kinematic service in the cloud, as well as two hardware products: the single-frequency GNSS data logger Argonaut specifically tuned for drone applications and photogrammetry, and its dual frequency evolution MEDEA GNSS computer. Despite the various market pivots that Rokubun has undergone during this journey, our plan went well up to a certain point, until it started to show some weaknesses."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#achievements",
    "href": "posts/farewell-rokubun/rokubun.html#achievements",
    "title": "On my journey as CTO at Rokubun",
    "section": "Achievements",
    "text": "Achievements\nWhile I risk sounding boastful, I‚Äôd like to share some of Rokubun‚Äôs achievements:\n\nWe managed to keep operations for 10 years, including a pandemic in the middle2.\nFrom 2 to 14 employees in 10 years with little external investment, mostly based on organic growth through revenues from contracts and grants.\nEarned a reputation within the navigation ecosystem. Professionals and colleagues in the sector know Rokubun, and I would venture to state that in a positive way: as a potential partner to be trusted. This helped us to build a strong professional network for the company to seek for contracts.\nBuilt a streamlined process to craft winning proposals. This included a strategy and plan to quickly seek partners and build a solid consortium for a tender/grant, generate valuable technical content that demonstrated our capabilities, and set up a methodology to ease and speed up the actual proposal writing in a collaborative way with our partners.\nAchieved a robust and company-wide software development strategy, a fundamental step to further achieve technical certifications such as ISO-26262, SOTIF, ‚Ä¶ This included a Continuous Integration and Continuous Deployment infrastructure based on Gitlab that was in the workflow of all staff. Even the ones in charge of research activities and responsibles of the company back office."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#why-we-had-to-close",
    "href": "posts/farewell-rokubun/rokubun.html#why-we-had-to-close",
    "title": "On my journey as CTO at Rokubun",
    "section": "Why we had to close?",
    "text": "Why we had to close?\nIn short, the main reason has been insufficient cashflow, which did not allow us to continue with operations. But, what caused this cashflow tension? There is not a single reason, but a combination of them, both external and internal. I would probably highlight the following:\n\nAs the famous adage says (reworded for the point in case): ‚ÄúOnce a consultancy, always a consultancy.‚Äù. Pivoting from a consultancy model to a product-based business proved to be more complex than we initially anticipated. While organic growth was part of the strategy, it ultimately limited resources dedicated to product development compared to established companies in the sector. Attracting talent and investment for product development became difficult as we lacked significant commercial traction.\nLack of certifications. Investing in software certification (ISO26262, SOTIF) was crucial for our long-term vision (for the type of product we wanted to develop and our target market). However, we have not been able to secure the necessary funds and time to cultivate a team mindset around achieving the certification. While initial steps were taken, achieving this goal would have required a more focused allocation of resources and a strategic team culture shift. Ultimately, this impacted our ability to access certain customers requiring specific certifications. This applied even for the process of tendering and grant preparation in various funding vehicles.\nAs a Spanish company, an uphill battle to access ESA‚Äôs NAVISP3 projects that we did not manage to win. This was totally out of our control, but something that severly limited our access to funds for ESA projects in the navigation sector, specially in the last years. As you might know, these projects require the approval of ESA‚Äôs National Delegations, but despite our efforts to revert the situation, we did not manage to change our odds in accessing funds for these projects."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#things-that-we-did-right",
    "href": "posts/farewell-rokubun/rokubun.html#things-that-we-did-right",
    "title": "On my journey as CTO at Rokubun",
    "section": "Things that we did right",
    "text": "Things that we did right\nDespite all this, I believe that Rokubun‚Äôs accomplishments can be largely attributed to the following:\n\nNetworking as a core activity in the company. This implied participate in conferences, fairs, hackathons and workshops. It seems a rather obvious activity to do in a company, but the truth is that day-to-day activities can make you underestimate its importance. Networking will be crucial to build partnerships for consortiums, requests for quotations, find potential customers, ‚Ä¶ This brings me to the following point, volunteer!.\nWe volunteered. A lot. Even if it implied some cost. Consider this cost as marketing expenses. This will help you build your brand and establish a positive reputation. Eventually, your efforts will pay off as your peers recognize your company‚Äôs ability to deliver and build trust. Some ideas: look for hackathons in your field and make yourself available to participate as mentor and, if cashflow allows, consider sponsorship. If you are on the technical side of things, volunteer to review scientific and technical journals and reach your network to inquire for opportunities to chair conference sessions. These activities can significantly increase your visibility and credibility.\nBe realistic on your odds to winning tenders, especially when you‚Äôre a small, early-stage company. Evaluators may perceive a higher risk in awarding projects to such firms. Consider partnering with larger companies to increase your chances of success.\nReaching the big guys. We have been always very proactive when talking to technical responsibles from the big companies in our sector. Technical workshops where a great opportunity to mingle with them and let them know the company. Moreover, you can always ‚Äúplay the SME card‚Äù: public tenders and grants usually foster collaboration with new actors and give extra points to consortiums in which SMEs have significant presence.\nCashflow management. As inexperienced managers as we were, specially at the beginning, one of the first (and best) decisions we took was to put in place a analytical cashflow tool. We used a simple spreadsheet for this, but we were strict and accurate on our monthly expenditure. This allowed us to make reliable predictions and properly plan ahead to estimate our runway. I know this is usual practice in company management, but it was something we were somewhat overlooking at the very beginning.\nUnderpromise and overdeliver when establishing objectives and performance figures in project proposals. You will have to find a sweet spot between what is feasible and and what is attractive for the customer to get the contract. Make sure you keep in reserve those nice-to-have features that entail a higher risk (of implementation, demonstration or test). Once you have the resources within the project you will be able to develop them and give an extra value to the project outcome. Even if these features are not realized in the end, you will still be successful in the project because the expectations of all parties will be aligned.\nCompensate low salaries with perks. At Rokubun we could not pay high salaries. As a consultancy with minimal investment, we did not have the resources to pay high wages. We were aware of this, and most importantly, employees were also aware of this. Rokubun compensated this limitation with flexible working hours, more holidays, a remote work policy and (we believe) engaging projects.\nFull transparency with Rokubun‚Äôs employees. Salary scales were public for all employees, salary raise criteria and objectives were also published in our internal Wiki page so everyone was aware of them. And we always tried to provide with a quantitative update on the achievement of each objective in (more or less) regular plenary meetings.\nFour eyes see more than two. The company operated on a consensus-based model between Xavi and myself. This ensured that we considered as many possible angles and consequences of our decisions. While we initially had concerns about a potential deadlock due to our equal shareholding, this never materialized. In fact, our shared ownership encouraged open dialogue and adaptability between us, which I believe ended up as a very positive asset for the company.\nAnd probably the most important point: we kept a huge whiteboard to write ideas, draw or sketch. I am really going to miss it."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#some-takeaways",
    "href": "posts/farewell-rokubun/rokubun.html#some-takeaways",
    "title": "On my journey as CTO at Rokubun",
    "section": "Some takeaways",
    "text": "Some takeaways\nThe following is a set of some topics and points to take into account for a small technological company in the technological sector:\n\nPlan for project evaluation periods. Keep in mind that the resolution of grants and tenders take time. In ESA contracts (tenders) it may take a couple of months, while EUSPA and European Commission may even take a bit longer. Make sure you secure your cashflow to sustain operations during this interim period. In a similar topic, revision and approval of interim reports may take as long as 6 months in certain institutions, so make sure your cashflow can withstand these time delays.\nIn ESA projects, if in two months there is no positive answer regarding a submitted proposal, it‚Äôs likely that the contract has been given to another consortium.\nI‚Äôve learned this elsewhere, but is very true: ‚ÄúThe disposition of a partner writing a proposal will be the disposition when executing a project‚Äù. Which means, if a partner is delayed providing contributions when preparing the proposal of the tender, expect delays when working in the project.\nPivoting, hardware is expensive and unforgiving, mistakes are costly.\n‚ÄúSweets that can turn bitter‚Äù. One of our service providers used these words to describe public funds. Be careful with them, no matter how attractive they may seem. Make sure you are perfectly aware of all the conditions. There will be clauses in the fund agreement that will pass unnoticed. This is a fact. My practical recommendation? treat public funds as some sort of credit that needs to be returned at some point. Plan your cashflow in a way that you need to return between 10% and 20% of the amount granted. In this way you will be prepared for any possible incidentals you made during the audit process.\nTeam management, if you are a CTO like myself, expect very short time spans of focused work. Avoid delegating tasks that disrupt the team‚Äôs focus: be prepared to do them yourself if need be, otherwise delays will happen. Make sure to do a good planning so that unexpected tasks can be smoothly integrated into the task pipeline of the team.\nConsultancy is seasonal as far as cash source is concerned. As soon as you get a project approved, start looking for the next opportunity. Consider also a line of credit despite its associated financial cost."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#final-words",
    "href": "posts/farewell-rokubun/rokubun.html#final-words",
    "title": "On my journey as CTO at Rokubun",
    "section": "Final words",
    "text": "Final words\nWorking at Rokubun has been certainly a stimulating experience: a rollercoaster. It has been a a source of worries some times but a constant flow of learning experiences and I value, in particular, the social aspect of this adventure: the people I met and the pleasure to work side-by-side with very talented professionals."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#ai-usage-in-this-post",
    "href": "posts/farewell-rokubun/rokubun.html#ai-usage-in-this-post",
    "title": "On my journey as CTO at Rokubun",
    "section": "AI usage in this post",
    "text": "AI usage in this post\nWhile I did use Gemini to polish some parts of the text (mostly style corrections and English typos), this post has been ideated, organized and written by me."
  },
  {
    "objectID": "posts/farewell-rokubun/rokubun.html#footnotes",
    "href": "posts/farewell-rokubun/rokubun.html#footnotes",
    "title": "On my journey as CTO at Rokubun",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAre you familiar with satellite navigation systems (GNSS) such as GPS or Galileo?‚Ü©Ô∏é\nThis was in part due to the nature of our work, that allowed us to temporarily move our operations to a fully remote approach. However, while I appreciate the flexibility of remote work, I still believe that face-to-face meetings and discussions are essential for effective collaboration, specially with more junior profiles. This applies even for software development tasks such as pair programming and even code reviews.‚Ü©Ô∏é\nESA NAVISP is the ESA programme for navigation-related projects.‚Ü©Ô∏é"
  },
  {
    "objectID": "games/status.html",
    "href": "games/status.html",
    "title": "Desenvolupament de jocs",
    "section": "",
    "text": "L‚Äôestat de desenvolupament de les diferents idees o projectes relacionats amb els jocs de taula que tinc entre mans est√† resumit en aquest diagrama que teniu abaix i, que, com no podia ser d‚Äôuna altra manera, l‚Äôhe fet amb Python i he incrustat el codi aqu√≠ abaix per si teniu curiositat.\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\n\nPROJECT_DATA = [\n  (\"Aventureiros √≥ tren, Galicia\", 0.4, \"Playtesting\"),\n  (\"C√≥digo Segredo\", 0.9, \"Primera versi√≥n completa, PnP na boardgamegeek\"),\n  (\"Cronocartas Historia de Galicia\", 0.7, \"En preproducci√≥n\"),\n  (\"Cronocartes Hist√≤ria de Catalunya\", 1.0, \"Disponible\"),\n\n]\n\n# Create a figure\nfig = go.Figure()\n\n# Set up bar parameters\nbar_width = 0.35\nbar_height = 0.1\ngap_between_bars = 0.05\nnum_bars = len(PROJECT_DATA)\n\n# Calculate bar positions and widths\nbar_positions = np.arange(num_bars) * (bar_width + gap_between_bars) + bar_width / 2\nbar_widths = [bar_width] * num_bars\n\n# Create foreground bars with rounded corners and slightly smaller width\nfor i in range(num_bars):\n    completion_rate = PROJECT_DATA[i][1]\n    hover_text = PROJECT_DATA[i][2]\n\n    fig.add_trace(go.Bar(\n        x=[completion_rate],\n        y=[bar_positions[i]],\n        width=bar_widths[i] * 0.8,\n        orientation='h',\n        marker=dict(color='#14a2ff', line=dict(width=1, color='#0072bd'), cornerradius=10),\n        hovertext=hover_text,\n        hoverinfo='text',\n    ))\n\n# Add project names within bars, left-justified\nfor i in range(num_bars):\n    project_name = PROJECT_DATA[i][0]\n    x = PROJECT_DATA[i][1]\n    fig.add_annotation(\n        x = 0.02,\n        y = bar_positions[i],\n        text = project_name,\n        showarrow = False,\n        xanchor=\"left\",\n        font=dict(color=\"white\", size=11),\n    )\n\n# Set up plot appearance\nfig.update_layout(\n    xaxis=dict(range=[0, 1], tickvals=[0, 0.25, 0.5, 0.75, 1], ticktext=['0%', '25%', '50%', '75%', '100%']),\n    yaxis=None,\n    #margin=dict(l=0, r=0, b=0, t=30),\n    showlegend=False,\n    autosize=False,\n    width=500,\n    height=200,\n)\nfig.update_yaxes(showticklabels=False)\nfig.layout.xaxis.fixedrange = True\nfig.layout.yaxis.fixedrange = True\nfig.select_annotations\n\n# Show the plot\nfig.show()"
  },
  {
    "objectID": "games/cronocartas/index.html",
    "href": "games/cronocartas/index.html",
    "title": "Cronocartas Historia de Galicia",
    "section": "",
    "text": "Descubre a historia de Catalu√±a xogando a Cronocartes, un xogo de cartas onde historiadores rivais competir√°n para demostrar os seus co√±ecementos sobre a historia de Galiza.\nCronocartas √© un xogo r√°pido e familiar: en s√≥ 30 segundos aprender√°s a xogar e as partidas adoitan durar menos de 20 minutos. Moitos adultos te√±en medo de xogar por vergo√±a a unha suposta ignorancia en temas hist√≥ricos, pero xa ver√°s que non hai nada de que preocuparse! ¬°Divertir√©desvos!"
  },
  {
    "objectID": "games/cronocartas/index.html#como-se-xoga",
    "href": "games/cronocartas/index.html#como-se-xoga",
    "title": "Cronocartas Historia de Galicia",
    "section": "Como se xoga",
    "text": "Como se xoga\nAtoparedes as instruci√≥ns nunha das cartas da mesma baralla:"
  },
  {
    "objectID": "games/cronocartas/index.html#preparaci√≥n",
    "href": "games/cronocartas/index.html#preparaci√≥n",
    "title": "Cronocartas Historia de Galicia",
    "section": "Preparaci√≥n",
    "text": "Preparaci√≥n\nSigue estes pasos para preparar a partida:\n\nBaralla ben as cartas e reparte 7 a cada historiador boca abaixo (pola cara que non mostra o ano).\nUnha vez repartidas as cartas, colle a carta superior da pila de cartas restante e pona boca arriba de tal maneira que o ano quede √° vista. Esta ser√° a carta inicial da li√±a temporal.\n\n¬°Xa est√°s a punto para comezar!"
  },
  {
    "objectID": "games/cronocartas/index.html#a-t√∫a-quenda",
    "href": "games/cronocartas/index.html#a-t√∫a-quenda",
    "title": "Cronocartas Historia de Galicia",
    "section": "A t√∫a quenda",
    "text": "A t√∫a quenda\nNa t√∫a quenda ter√°s que coller unha das t√∫as cartas que estean boca abaixo e colocala na li√±a do tempo, onde penses que respecta a orde cronol√≥xica. Despois de decidirte, d√°lle a volta √° carta para que se vexa o ano. Neste punto poden ocorrer d√∫as cousas:\n\nA carta respecta a orde cronol√≥xica (ben feito!üëè), deixa a carta na li√±a do tempo.\nA carta non respecta a orde cronol√≥xica (estabas equivocado!üò•), descarta a carta e colle outra do mont√≥n de cartas.\n\nUnha vez resolto un destes casos, a quenda pasa ao seguinte historiador."
  },
  {
    "objectID": "games/cronocartas/index.html#final-do-xogo",
    "href": "games/cronocartas/index.html#final-do-xogo",
    "title": "Cronocartas Historia de Galicia",
    "section": "Final do xogo",
    "text": "Final do xogo\nO momento en que un/a historiador/a coloca correctamente a s√∫a √∫ltima carta dispara o final do xogo. S√©guese xogando ate que acabe a ronda (para que todos os/as historiadores/as poidan xogar o mesmo n√∫mero de quendas).\nAo final da √∫ltima ronda, o xogador que se quedou sen cartas ga√±a a partida!\nNo caso de que na √∫ltima ronda haxa un empate, e m√°is dun/a historiador/a haxa acabado as s√∫as cartas, a vitoria decidirase por morte s√∫bita: ir√°n xogando cartas e os/as finalistas quedar√°n eliminados no caso de colocar incorrectamente unha carta."
  },
  {
    "objectID": "games/cronocartas/index.html#variante-avanzada",
    "href": "games/cronocartas/index.html#variante-avanzada",
    "title": "Cronocartas Historia de Galicia",
    "section": "Variante avanzada",
    "text": "Variante avanzada\nSe queres facer o xogo un pouco m√°is dif√≠cil, en vez de descartar a carta no caso de equivocaci√≥n, col√≥caa correctamente na li√±a temporal."
  },
  {
    "objectID": "games/cronocartas/index.html#estratexia",
    "href": "games/cronocartas/index.html#estratexia",
    "title": "Cronocartas Historia de Galicia",
    "section": "Estratexia",
    "text": "Estratexia\nUn consello: a√≠nda que esteas tentado a colocar primeiro as cartas que sabes, tes que facer precisamente o contrario! Xoga primeiro as que son m√°is dubidosas. Se non o fas as√≠, √° medida que a li√±a temporal v√°iase poboando de cartas, serache m√°is dif√≠cil afinar o ano e a probabilidade de equivocarse ser√° m√°is alta."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "",
    "text": "GNSS Expert | Tech Transfer and Innovation | Software Development"
  },
  {
    "objectID": "index.html#expertise",
    "href": "index.html#expertise",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "Expertise",
    "text": "Expertise\nMy technological expertise covers\n\nGlobal Navigation Satellite Systems technology, from algorithm definition to implementation. My main expertise covers Navigation algorithms as well as ionospheric monitoring using GNSS.\nGNSS data analysis\nTerrestrial and Alternate Position, Navigation and Timing (PNT) systems (e.g.¬†Wi-Fi Round Trip Time and other range-based systems such as 5G and Ultrawide-Band)\nSoftware development (from proof-of-concept to production based systems based on Continuous Integration and Continuous Deployment)\n\nOther relevant skills\n\nTechnology transfer and Innovation\nEU tender and grant proposal preparation and evaluation\nProject management"
  },
  {
    "objectID": "index.html#short-bio",
    "href": "index.html#short-bio",
    "title": "Miquel Garcia-Fernandez, PhD",
    "section": "Short bio",
    "text": "Short bio\nI am a GNSS Research Engineer with over 20 years of experience. I holds a BSc in Telecommunication Engineering (1999) and a PhD in GNSS Data Processing for Ionospheric Monitoring (2004) from the Polytechnic University of Catalonia (Spain).\nI have a diverse career spanning academia and industry, and have held key positions at renowned institutions, including:\n\nRokubun: CTO and co-founder position of Rokubun\nJPL/NASA: Technologist positgion, doing GNSS data processing for navigation, geodesy and orbit determination, as well as implementing physical models for the next generation GPS control segment\nStarlab Barcelona: Space Program and Area Manager, executing projects related to GNSS data processing for Earth observation as well as GNSS receiver development.\nDLR: Scientific saff at the GNSS Technology and Navigation group of the German Aerospace Center (DLR), developing GNSS systems for LEO applications.\nUniversity of Kyoto: Post-doctoral position for one year in the Research Center of Sustainable Humanosphere (RISH) on data combination for ionospheric monitoring,\n\nI am also an associate editor in the GPS Solutions journal."
  },
  {
    "objectID": "games/torneig_cronocartes.html",
    "href": "games/torneig_cronocartes.html",
    "title": "CronoTorneig",
    "section": "",
    "text": "Aquesta p√†gina inclou notes, idees i instruccions per muntar un torneig de Cronocartes. En particular, es defineixen regles de puntuaci√≥, desempats i organitzaci√≥ de rondes."
  },
  {
    "objectID": "games/torneig_cronocartes.html#organitzaci√≥-b√†sica",
    "href": "games/torneig_cronocartes.html#organitzaci√≥-b√†sica",
    "title": "CronoTorneig",
    "section": "Organitzaci√≥ b√†sica",
    "text": "Organitzaci√≥ b√†sica\nPer un torneig en el qual participen 16 historiadors:\n\nA cada partida hi competiran 4 historiadors (necessiteu doncs 4 taules)\nUn cop hi hagi guanyador, la partida continuar√† fins que tots els jugadors col¬∑loquin les seves cartes. Aquest pas √©s important perqu√® cal saber qui queda 1r, 2n, 3r i 4t. Les puntuacions de cada historiador seran les seg√ºents:\n\n1r classificat: 5 punts\n2n classificat: 3 punts\n3r classificat: 2 punts\n4t classificat: 1 punts\n\nEls empats per qualsevol posici√≥ es desfaran per mort sobtada: cada historiador rebr√† una carta i quedar√† eliminat en cas que s‚Äôequivoqui al posar la carta a la l√≠nia temporal ja existent de la partida en curs.\nEl torneig s‚Äôestructurar√† en un total de 5 rondes distribu√Ødes de la seg√ºent manera:\n\nRondes classificat√≤ries: 3 rondes en format su√≠s. D‚Äôuna ronda a la seg√ºent, els historiadors s‚Äôagruparan per puntuaci√≥: els 4 primers aniran a una taula, els 4 segons a una altra i aix√≠ successivament. D‚Äôaquesta manera, si un historiador ha tingut una mala partida, encara tindr√† opcions de guanyar.\nRondes eliminat√≤ries: semi-final i final. Per la semi-final, s‚Äôorganitzaran les taules de manera ponderada: la primera taula estar√† constitu√Øda pel 1r, 4t, 5√® i 8√® classificat, i a la segona taula hi seran el 2n, 3r, 6√® i 7√®. El motiu √©s per tenir dues taules homog√®nies en tant que nivell dels historiadors. Finalment, la taula de la ronda final estar√† constitu√Øda pels dos primers classificats de cada taula de semi-finals."
  },
  {
    "objectID": "games/torneig_cronocartes.html#empat-a-la-gran-final",
    "href": "games/torneig_cronocartes.html#empat-a-la-gran-final",
    "title": "CronoTorneig",
    "section": "Empat a la gran final",
    "text": "Empat a la gran final\nEn cas que hi hagi un empat entre els finalistes i totes les cartes s‚Äôhagin esgotat (com va passar a la final del 2n torneig Cronomaster del Museu d‚ÄôHist√≤ria de Catalunya), aqu√≠ teniu algunes idees:\n\nFeu una partida al Cronocartas Historia de Galicia\nEliminat√≤ria per batalla de Burritos\nLlen√ßar una moneda\nPartida d‚Äôescacs\n\nSi tot aix√≤ falla, sempre quedar√† una bona llen√ßada de daus. üòú"
  },
  {
    "objectID": "games/torneig_cronocartes.html#altres-consideracions",
    "href": "games/torneig_cronocartes.html#altres-consideracions",
    "title": "CronoTorneig",
    "section": "Altres consideracions",
    "text": "Altres consideracions\n\n‚ÄúHandicap‚Äù. Es repartiran cartes addicionals a guanyadors d‚Äôedicions anteriors del torneig: una carta addicional per cada edici√≥ anterior en qu√® ha resultat guanyador. Exemple: En Sever√≠ ha guanyat el 1r i el 2n torneigs de Cronocartes, per tant, a la 3a edici√≥ del torneig comen√ßar√† amb 9 cartes, en comptes de 7.\nPartides de tres historiadors: Repartiu 8 cartes en comptes de 7 per mantenir el nivell de dificultat similar que en partides de 4 historiadors amb 7 cartes. Per exemple, amb 4 historiadors hi ha 28 esdeveniments (\\(4 \\cdot 7 = 28\\)), mentre que amb 3 historiadors, si es reparteixen 8 cartes, n‚Äôhi haur√† 24 (\\(3 \\cdot 8 = 24\\))."
  },
  {
    "objectID": "games/torneig_cronocartes.html#agra√Øments",
    "href": "games/torneig_cronocartes.html#agra√Øments",
    "title": "CronoTorneig",
    "section": "Agra√Øments",
    "text": "Agra√Øments\nGr√†cies a Monica Mallo i Guillem Guasch per la revisi√≥ del contingut d‚Äôaquesta p√†gina."
  },
  {
    "objectID": "games/cronocartes/index.html",
    "href": "games/cronocartes/index.html",
    "title": "Cronocartes Hist√≤ria de Catalunya",
    "section": "",
    "text": "Descobreix la hist√≤ria de Catalunya a trav√©s del joc de cartes, Cronocartes, on historiadors rivals competiran per demostrar el seu coneixement de la hist√≤ria de Catalunya.\nCronocartes √©s un joc de cartes r√†pid i familiar: nom√©s necessiteu 30 segons per aprendre a jugar i les partides no acostumen a durar m√©s de 20 minuts. Molts adults tenen por de jugar per vergonya a una suposada ignor√†ncia en temes hist√≤rics, per√≤ ja veureu com no n‚Äôhi ha per a tant! Us divertireu!"
  },
  {
    "objectID": "games/cronocartes/index.html#com-es-juga",
    "href": "games/cronocartes/index.html#com-es-juga",
    "title": "Cronocartes Hist√≤ria de Catalunya",
    "section": "Com es juga",
    "text": "Com es juga\nTrobareu les instruccions en una de les cartes de la mateixa baralla:\n\n\n\n\n\n\n\n\n\n\n\nPreparaci√≥\nSeguiu aquests passos per preparar la partida:\n\nMescleu b√© les cartes i repartiu-ne 7 a cada historiador cara avall (per la cara que no mostra l‚Äôany).\nUn cop repartides les cartes, preneu la carta superior de la pila de cartes restant i poseu-la cara amunt de tal manera que l‚Äôany ha de quedar a la vista. Aquesta ser√† la carta inicial de la l√≠nia temporal.\n\nJa esteu a punt per comen√ßar!\n\n\nEl teu torn\nEn el teu torn haur√†s d‚Äôagafar una de les teves cartes que estan cara avall i col¬∑locar-la a la l√≠nia temporal, on creguis que respecta l‚Äôordre cronol√≤gic. Despr√©s d‚Äôhaver-te decidit, gira la carta. En aquest moment poden passar dues coses:\n\nLa carta respecta l‚Äôordre cronol√≤gic (ben fet!üëè), deixa la carta a la l√≠nia temporal.\nLa carta no respecta l‚Äôordre cronol√≤gic (t‚Äôhas equivocat!üò•), descarta la carta i pre-ne una altra de la pila de cartes.\n\nUn cop hagis resolt un d‚Äôaquests casos, passa el torn al/la seg√ºent historiador/a.\n\n\nFinal del joc\nEn el moment en qu√® un/a historiador/a col¬∑loca correctament la seva √∫ltima carta, es dispara el final del joc i s‚Äôacaba la ronda (per tal que tots els/les historiadors/es hagin jugat el mateix nombre de torns).\nAl final de l‚Äô√∫ltima ronda, el jugador que s‚Äôhagi quedat sense cartes guanya la partida!\nEn cas que a l‚Äô√∫ltima ronda hagi hagut un empat i m√©s d‚Äôun/a historiador/a hagi acabat les seves cartes, la vict√≤ria es decidir√† per mort sobtada: s‚Äôaniran jugant cartes i els/les finalistes quedaran eliminats en cas de col¬∑locar incorrectament una carta.\n\n\nVariant avan√ßada\nSi voleu fer el joc una mica m√©s dif√≠cil, en comptes de descartar la carta en cas d‚Äôequivocaci√≥, col¬∑loqueu-la correctament a la l√≠nia temporal.\n\n\nEstrat√®gia\nUn consell: tot i que esteu temptats a col¬∑locar primer les cartes que sabeu, heu de fer precisament el contrari! Jugueu primer les que s√≥n m√©s dubtoses. Si no ho feu aix√≠, a mesura que la l√≠nia temporal es vagi poblant de cartes, us ser√† m√©s dif√≠cil afinar l‚Äôany i la probabilitat d‚Äôequivocar-se ser√† m√©s alta."
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html",
    "href": "posts/ticket-to-ride-graph-analysis/index.html",
    "title": "Graph theory to model Ticket to ride",
    "section": "",
    "text": "This post contains a basic analysis of the mathematical model behind the board game Ticket to Ride by leveraging graph theory.\nAs you know, ‚ÄúTicket to Ride‚Äù is a game where players aim at completing train routes in the most efficient way possible. The first version of the game is set in the United States of America in the late 19th century during the railroad expansion.\nModeling the game is understood as assessing the distribution of:"
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html#analysis",
    "href": "posts/ticket-to-ride-graph-analysis/index.html#analysis",
    "title": "Graph theory to model Ticket to ride",
    "section": "Analysis",
    "text": "Analysis\nTo do this analysis, we have used following key libraries:\n\nPandas for data loading and processing\nNetworkX for graph processing and above all, to find the shortest path between two stations (nodes).\n\nFirst we will need to install some required libraries as well as import necessary modules\n\n!pip install matplotlib networkx pandas\n\n\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport pandas as pd\n\nThe following cells contain some constants used in the notebook as well as the links to the CSVs that contain the Tracks and routes\n\nTRACK_LENGTH_COLUMN = 'length'\nFROM_COLUMN = 'from'\nTO_COLUMN = 'to'\n\nTRACKS_CSV_FILE = 'Ticket_to_ride - Tracks.csv'\nTICKETS_CSV_FILE = 'Ticket_to_ride - Tickets.csv'\n\nFirst let‚Äôs load the Track CSV, this will be used to visualize the track network as a mathetmatical graph (of nodes/stations and edges/tracks). The CSV contains the two stations for each track as well as their associated color and lenghts (number of wagons required to complete the track)\n\ndf = pd.read_csv(TRACKS_CSV_FILE)\ndf[TRACK_LENGTH_COLUMN] = pd.to_numeric(df[TRACK_LENGTH_COLUMN])\ndf.head()\n\n\n\n\n\n\n\n\nfrom\nto\ncolor\nlength\n\n\n\n\n0\nWashington\nNew York\nblack\n2\n\n\n1\nPittsburgh\nChicago\nblack\n3\n\n\n2\nRaleigh\nNashville\nblack\n3\n\n\n3\nDuluth\nWinnipeg\nblack\n4\n\n\n4\nKansas City\nDenver\nblack\n4\n\n\n\n\n\n\n\nWith this DataFrame we can know useful information such as:\n\nNumber of total tracks\nNumber of wagons required for each color\nTrack distribution for each color\n\nIn addition, we can use the DataFrame to create the graph using the networkx library and make a visual representation of the track network\n\n# Create a directed graph with weighted edges\ngraph = nx.from_pandas_edgelist(df, FROM_COLUMN, TO_COLUMN, edge_attr=TRACK_LENGTH_COLUMN, create_using=nx.Graph())\n\n# Plot the graph\npos = nx.spring_layout(graph, weight=TRACK_LENGTH_COLUMN, seed=4)\nnx.draw(graph, pos, with_labels=True, node_size=400, node_color='skyblue', font_size=9, font_color='black', font_weight='bold', edge_color='grey', linewidths=2)\n\n# Add edge labels showing weights\nedge_labels = nx.get_edge_attributes(graph, TRACK_LENGTH_COLUMN)\nnx.draw_networkx_edge_labels(graph, pos, edge_labels=edge_labels)\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\nThe advantage of modeling the track network as a graph is that we can use already existing libraries to compute the shortest path between two stations (both the passing stations as well as the total length), as in the following example\n\nFROM = 'El Paso'\nDESTINATION = 'Raleigh'\n\nshortest_path = nx.shortest_path(graph, source=FROM, target=DESTINATION, weight=TRACK_LENGTH_COLUMN)\nprint(f'The shortest path goes through these stations: {\" - \".join(shortest_path)}' )\n\nshortest_path_length = nx.shortest_path_length(graph, source=FROM, target=DESTINATION, weight=TRACK_LENGTH_COLUMN)\nprint(f'The length for the shortes path is: {shortest_path_length}')\n\nThe shortest path goes through these stations: El Paso - Dallas - Little Rock - Nashville - Raleigh\nThe length for the shortes path is: 12\n\n\nThe shortest path goes through these stations: El Paso - Dallas - Little Rock - Nashville - Raleigh\nThe length for the shortes path is: 12\nThese methods (specially nx.shortest_path_length) are very relevant to reverse engineer the ticket cards (that give points if a player completes a route). Basically we need to know how the points awarded per card is related to the shortest path.\nTo do so, we will load the Ticket CSV into a DataFrame and compute the shortest path for each card:\n\n# Load the Ticket CSV\ndf_tickets = pd.read_csv(TICKETS_CSV_FILE)\n\n# Create a new column in the dataframe with the shortest path length for each route (i.e. ticket card)\ndf_tickets[TRACK_LENGTH_COLUMN] = df_tickets.apply(lambda r: nx.shortest_path_length(graph, source=r[FROM_COLUMN], target=r[TO_COLUMN], weight=TRACK_LENGTH_COLUMN), axis=1)\n\ndf_tickets.head()\n\n\n\n\n\n\n\n\nfrom\nto\npoints\nlength\n\n\n\n\n0\nLos Angeles\nNew York\n21\n20\n\n\n1\nWinnipeg\nLittle Rock\n11\n11\n\n\n2\nVancouver\nSanta Fe\n13\n13\n\n\n3\nMontreal\nAtlanta\n9\n9\n\n\n4\nToronto\nMiami\n10\n10\n\n\n\n\n\n\n\nAt this point we can realise that the points awarded per route are in fact equivalent (at least on the most cases) with the lenghts (i.e.¬†total number of wagons required to complete the route), which makes sense.\n\nplt.plot(df_tickets[TRACK_LENGTH_COLUMN], df_tickets['points'], 'o')\nplt.xlabel('Shortest route length [number of wagons]')\nplt.ylabel('Ticket card points')\nplt.title('Relationship between Ticket card points and\\nShortest route length')\nplt.show()"
  },
  {
    "objectID": "posts/ticket-to-ride-graph-analysis/index.html#key-takeaways",
    "href": "posts/ticket-to-ride-graph-analysis/index.html#key-takeaways",
    "title": "Graph theory to model Ticket to ride",
    "section": "Key takeaways",
    "text": "Key takeaways\nFrom the analysis, the following takeaways are relevant in case you need to make your own customization of Ticket to ride:\n\nThere are 36 stations\nThere are a total of 100 tracks (double tracks are considered independent tracks)\nEach color (except grey) needs a total of 27 wagons to complete their tracks\nEach color (except grey) has a total number of 7 tracks, and their distribution is: 6-5-4-4-3-3-2, except for green and white, for which their distribution is: 6-5-5-4-3-2-2.\nTo the largest possible extent, the colors are homogeneously distributed on the territory (which makes sense in order to balance the game).\nThere are 30 ticket cards\nThe points for each ticket card represent the shortest path length (i.e.¬†minimum number of wagons required to complete the card), with some small exceptions."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html",
    "href": "posts/gnss_data_data_volume/index.html",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "",
    "text": "For those of you who work with GNSS data processing, have you ever wondered how much data do servers such as CDDIS or EUREF, who store historic GNSS raw measurement data, need to handle?\nIf your use case is to perform a positioning session for a single receiver maybe not, but I bet the situation is different if you are building GNSS products (e.g.¬†estimation of GNSS satellite orbits and clocks, ionospheric monitoring, ‚Ä¶), where a worldwide distribution of receivers (and data) is not only desirable but mandatory.\nIn order to answer this question, I have implemented a script that lists the files stored in the GNSS raw measurement data folders provided CDDIS server. In particular, I selected the daily files with 30 seconds sampling rate. In order not to overwhelm the servers, just one day for each year (January 1st) was processed. Then, the code included below in this post has been used to plot the number of (unique) stations1 as well as the median file size. The total folder size has been also computed.\nInitially, one may think that the volume increase could be due to two factors:\nCode\nimport datetime\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfiles = glob.glob('./data/folder_*.txt')\n\n# List of all dataframes\ndfs = []\n\n# load the data files\nfor f in files:\n\n    date = datetime.datetime.strptime(f, './data/folder_%Y_%j.txt')\n    data = pd.read_csv(f, delimiter=\"\\s+\", names=['file', 'size'])\n    data['date'] = date\n    data['station'] = data['file'].str[:4].str.lower()\n    dfs.append(data)\n\n# Aggregate all dataframes\ndf = pd.concat(dfs)\n\nstats = df.groupby('date').agg(\n    n_stations=('station', lambda x: x.nunique()),\n    median_size=('size', 'median'),\n    total_size=('size', 'sum'))\n\n\nfig, ax1 = plt.subplots(figsize=(8, 4))\n\n# colors\nblue = '#0072bd'\nred = '#a2142f'\n\nplt.title(\"Evolution of GNSS data folder size\\nCDDIS network (daily 30s data files)\")\n\nax1.plot(stats.index, stats['n_stations'], label=\"Number of stations\", c=blue)\n\nax2 = ax1.twinx()\nax2.plot(stats.index, stats['median_size'] / 1.0e6, label=\"Median file size [Mb]\", linestyle='--', c= red)\nax2.plot(stats.index, stats['total_size'] / 1.0e9, label=\"Total folder size [Gb]\", c= red)\n\nax1.set_ylabel('Number of stations [count]', color=blue)\nax2.set_ylabel('File size [Mb] or Folder size [Gb]', color=red)\nax1.tick_params(axis='y', colors=blue)\nax2.tick_params(axis='y', colors=red)\n\nax1.legend(loc='upper left')\nax2.legend(loc='lower right')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nGNSS data volume over years for CDDIS network\nBut looking at the figure is clear that the main driving factor for the volume size increase is the larger file size: note that from 2012 the number of stations has stabilized and yet the total size has suffered a large increase (following the trend of the file size). Note also that most of the folders contain duplicated data: data from the same station might be stored in both RINEX v2 and v3 (althought this may change in the future, as V2 support is winding down).\nIn order to illustrate the causes of the file size increase, let‚Äôs take a particular station (EBRE, Observatori de l‚ÄôEbre, Spain) as an example. All files studied are compressed using Hatanaka and binary zip. The constellations and bands tracked in 4 particular files in the past are summarized in the following table:\nIt is clear then that the qualitative jump between 2012 to 2016 is caused by the recording of additional constellations (most notably Galileo and Beidou, which have associated many more signals). Additional increase in the file size is the inclusion of Doppler observables (in the case of EBRE sometime between 2016 and 2018)."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#conclusions",
    "href": "posts/gnss_data_data_volume/index.html#conclusions",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the analyzed data, several key observations emerge:\n\nData Volume: For the CDDIS network, the daily data volume currently hovers around 2.5 GB, translating to approximately 1 TB/year. This figure excludes high-rate datasets recorded at 1-second intervals, which significantly increase the overall volume (at least by a factor of 30).\nStation Count: The number of stations within the CDDIS network has remained relatively stable at around 500 since 2014, which discards this parameter as the main cause for the volume increase.\nFile size: is the actual responsible of the overall folder size increase (rather than the station count). A notable surge in file and folder sizes started occuring in 2016, primarily due to the introduction of new constellations, associated signals, and the inclusion of additional observables such as e.g.¬†Doppler observables.\n\nThe substantial increase in data volume poses challenges beyond mere storage considerations. The RINEX format, while human-readable, is not optimized for machine processing, potentially hindering applications that require the analysis of large datasets (Big data, AI/ML, ‚Ä¶). To address this limitation, alternative binary (machine-friendly) formats should be considered as a complementary solution to enhance processing efficiency: a preliminary analysis show that while loading (parsing) a RINEX format in Python may take several tens of seconds (for a 5h 1-second interval RINEX V3 file), the same file stored in Apache Parquet takes little more than a second to load."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#acknowledgements",
    "href": "posts/gnss_data_data_volume/index.html#acknowledgements",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to NASA (and CDDIS) to provide the data used to elaborate this blog post."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#use-of-ai",
    "href": "posts/gnss_data_data_volume/index.html#use-of-ai",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial Intelligence has been used to polish some text styling and correct some typos. Ideation and data processing has been done by the author."
  },
  {
    "objectID": "posts/gnss_data_data_volume/index.html#footnotes",
    "href": "posts/gnss_data_data_volume/index.html#footnotes",
    "title": "On the exploding size of GNSS measurement data files",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe same folder may contain data for the same receiver with different formats (e.g.¬†RINEX v2 and RINEX v3), the script below makes sure only unique stations are computed.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/codigo-segredo/codigo_segredo.html",
    "href": "posts/codigo-segredo/codigo_segredo.html",
    "title": "Traduci√≥n do xogo ‚ÄúC√≥digo Secreto‚Äù √≥ galego",
    "section": "",
    "text": "Seguro que moitos de v√≥s co√±ecedes o xogo de mesa ‚ÄúC√≥digo Secreto‚Äù, no que dous equipos de esp√≠as tratan de localizar os seus axentes de campo usando palabras. Como √© unha carreira para ver que equipo completa antes esta tarefa, √© crucial usar pistas que agrupen ou asocien m√∫ltiples palabras clave que identifican os nosos axentes de campo.\n\n\n\nC√≥digo Segredo\n\n\n\nSaberedes seguramente que existen variantes de este xogo en diversos idiomas, a castel√°n por suposto, pero tam√©n hai a versi√≥n comercial en catal√°n. Como non vin a versi√≥n en galego me aventurei a facela, e neste periplo aprend√≠n un par de ideas que quizais podan ser √∫tiles a quen queira crear expansi√≥ns a outros idiomas como o √©uscaro, suahili ou casaco. Por que non nos enganemos, traducir o ‚ÄúC√≥digo Segredo‚Äù non √© simplemente traducir literalmente as palabras: hai que trasladar tam√©n o contexto das palabras. Cando te po√±as a traballar, ten en conta estas ideas:\n\nPolisemia e homograf√≠a. En ‚ÄúC√≥digo Segredo‚Äù √© crucial que haxan palabras con m√∫ltiples significados para que os xogadores podan buscar asociaci√≥ns de termos. O problema √© que en linguas que derivan da mesma ra√≠z (por exemplo o lat√≠n), os varios significados que pode agrupar una palabra nun idioma p√≥dense reducir noutro. Por exemplo, ‚Äútaula‚Äù en catal√°n significa mesa e t√°boa (en galego), pero a voz galega ‚Äúmesa‚Äù perde o significado de ‚Äút√°boa‚Äù, as√≠ que teremos que buscar una alternativa.\nContexto e termos culturais. Si tedes a versi√≥n catalana do xogo, atoparedes termos como ‚ÄúMontserrat‚Äù, ‚Äúcaganer‚Äù ou ‚Äúti√≥‚Äù, termos que levan un vencello moi forte ca cultura catalana pero que non existen en galego. A mais, usar termos culturalmente ligados a lingua far√° que a experiencia de xogo sexa moito mais divertida. No caso galego, p√≥dese usar alternativas como ‚ÄúAncares‚Äù, ‚ÄúTerra Ch√°‚Äù, ‚ÄúLobishome‚Äù ou ‚ÄúSanta Compa√±a‚Äù.\n\n\n\n\nMostra de cartas de palabras\n\n\n¬øTedes algunha outra idea sobre este tema que queirades propo√±er?\nDeixo aqu√≠ a ligaz√≥n do ‚ÄúPrint and Play‚Äù para que podades imprimir vos mesmos as cartas de palabras e xogar a versi√≥n galega (s√≥ precisades una copia do xogo en calquera outro idioma dispo√±√≠bel)."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html",
    "href": "posts/parquet_format_for_gnss_measurements/index.html",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "",
    "text": "Following up on a previous idea regarding the sheer volume of the GNSS measurements generated by CORS1 networks, this post explores a potential binary storage of GNSS measurements (rather than text) leveraging the Apache parquet format. This format is highly used in Artificial Intelligence and Machine Learning applications, which is being also adopted in GNSS for e.g.¬†prediction of precise product (orbits and clocks), see also Siemuri et al. (2022) or Mao et al. (2024). By transitioning from text-based storage to a binary format like Parquet, we can unlock significant speed improvements in GNSS data processing pipelines, eliminating the need for time-consuming text parsing.\nFrom this post, you can expect the following:"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#converting-rinex-to-parquet",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#converting-rinex-to-parquet",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Converting RINEX to parquet",
    "text": "Converting RINEX to parquet\nConverting RINEX files to the Parquet format involves a two-step process:\n\nParsing the RINEX data: We leverage the roktools library to efficiently parse the RINEX file and organize the data into a structured format.\nStorage with pandas: Once parsed, the data is seamlessly converted into a pandas DataFrame. Pandas, a popular Python library for data manipulation, offers a built-in function for writing DataFrames directly to Parquet format.\n\nThis approach simplifies the conversion process and is shown in the code snippet below:\n\nimport tempfile\nimport pandas as pd\nfrom roktools import rinex\n\n# we use Context Manager to store the parquet file in a\n# temporary file that will be cleaned up automatically\n# after loading it\nwith tempfile.NamedTemporaryFile() as fh:\n\n    rinex.to_parquet(['SUN600SWE_S_20241312200_01M_01S_MO.rnx'], output_filename=fh.name)\n\n    # Rewind temporary file\n    fh.seek(0)\n\n    # Load the parquet file into a DataFrame\n    df = pd.read_parquet(fh.name)\n\n# Print preview of the DataFrame\ndf.head()\n\n\n\n\n\n\n\n\nepoch\nconstellation\nsat\nchannel\nsignal\nrange\nphase\ndoppler\nsnr\nslip\nstation\n\n\n\n\n0\n2024-05-10 22:00:00\nG\nG04\n1C\nG041C\n2.462384e+07\n1.293992e+08\n-3646.251\n35.0\n0\nsun6\n\n\n1\n2024-05-10 22:00:00\nG\nG04\n2W\nG042W\n2.462385e+07\n1.008308e+08\nNaN\n20.0\n0\nsun6\n\n\n2\n2024-05-10 22:00:00\nG\nG04\n2X\nG042X\n2.462385e+07\n1.008308e+08\nNaN\n41.0\n0\nsun6\n\n\n3\n2024-05-10 22:00:00\nG\nG04\n5X\nG045X\n2.462385e+07\n9.662954e+07\nNaN\n45.0\n0\nsun6\n\n\n4\n2024-05-10 22:00:00\nG\nG04\n1X\nG041X\n2.462384e+07\n1.293995e+08\nNaN\n37.0\n0\nsun6\n\n\n\n\n\n\n\nAs you can see, the data has a columnar layout with the following fields:\n\nepoch of the measurement (corresponds to the RINEX epoch of the measurements)\nconstellation: single character with the constellation (RINEX convention, e.g.¬†R for Glonass, G for GPS, ‚Ä¶)\nsat: Three character satellite identifier. First letter corresponds to the constellation and the last two characters corresponds to the satellite number (as defined by the RINEX format).\nchannel: two character description of the tracking channel. The values from this field correspond to the last two characters of the three-character RINEX channel code. The first character (the observable type) has been dropped because all measurements associated to this tracking channel (code, phase, Doppler and C/N0) are placed in the same row. This layout makes it straightforward to compute observables such as code-minus-carrier.\nsignal: Union of the satellite and channel fields. Albeit this may seem a redundant field, some optimization considerations make this field useful, as explained in the section below.\nrange: Pseudorange in meters\nphase: Carrier-phase in cycles\ndoppler: Doppler observables expressed in cycles per second\nsnr: C/N0 expressed in dB-Hz\nslip: Cycle slip / loss of lock field of the RINEX format.\nstation: Name of the station that recorded the measurements. Having this field may allow to have observables from multiple stations in the same parquet file."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-compute-the-code-minus-carrier-observable",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-compute-the-code-minus-carrier-observable",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: compute the code-minus-carrier observable",
    "text": "Tip: compute the code-minus-carrier observable\nWith the column layout proposed for the parquet format, extracting observables such as the CMC (code-minus-carrier), that use data for the same tracking channel, becomes straightforward:\n\nimport matplotlib.pyplot as plt\n\nsignal = 'G041C'\n\n# Filter the data to extract only the desired signal\ndf_signal = df[df['signal'] == signal]\n\n# Elapsed time\nt0 = df_signal['epoch'].loc[0]\nx = (df_signal['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# CMC (conversion from cycles to meters is required)\nwavelength_l1 = 299792458/(154.0*10.23e6)\ny = df_signal['range'] - df_signal['phase'] * wavelength_l1\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('Code-minus-carrier [m]')\nplt.title(f'Code-minus-carrier combination for signal {signal}')\nplt.plot(x, y, '.-')\n\n\n\n\nExample of code-minus-carrier combination computation"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-groupby-to-efficiently-process-data",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-groupby-to-efficiently-process-data",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: use groupby to efficiently process data",
    "text": "Tip: use groupby to efficiently process data\nUsually, a GNSS analyst processes the data on a satellite-station (e.g.¬†link) basis or on a signal-basis (satellite and tracking channel). Examples are:\n\nDetection of code outliers or single-frequency cycle slip detection based on jumps in the CMC combination.\nComputation of Rate of Total Electron Content Index (ROTI) for scintillation monitoring (Pi et al. (1997))\nComputation of multi-channel observations such as ionospheric free or geometry free combinations\n\nIn those cases, when using pandas, the groupby strategy becomes in handy to quickly work on a signal-per-signal basis in a very efficient manner\n\n\n\n\n\n\nTipAvoid for loops\n\n\n\nLeveraging techniques like groupby enables efficient vectorized operations, significantly outperforming slow Python for loops.\n\n\nAs an example, let‚Äôs compute the first derivate of the phase (for all signals) and compare it with the Doppler observable (please note that no for has been used in the code):\n\n# Compute the time derivative of the phase\ndf['d_phase'] = df.groupby('signal')['phase'].diff()\n\n\n\nCode\nsignal = 'G041C'\n\ndf_signal = df[df['signal'] == signal]\n\n# Time elapsed\nt0 = df_signal['epoch'].loc[0]\nx = (df_signal['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('Doppler and phase time derivative [cycles/s]')\nplt.title(f'Time derivative of signal {signal}')\nplt.plot(x, df_signal['d_phase'], '.-', label=\"d phase/dt\")\nplt.plot(x, -df_signal['doppler'], '.-', label=\"Doppler\")\nplt.legend(loc=\"lower right\")\n\n\n\n\n\nTime derivative of the carrier phase"
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-merge-to-compute-multifrequency-observables",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#tip-use-merge-to-compute-multifrequency-observables",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Tip: use merge to compute multifrequency observables",
    "text": "Tip: use merge to compute multifrequency observables\nAnother powerful method to process GNSS data, specially when trying to combine data from different signal, is the database-inherited merge method. This allows to join sections of the DataFrame and quickly build dual-frequency combinations. To illustrate this, let‚Äôs compute the geometry-free (or ionospheric) combination.\n\nchannel_a = '1X'\nchannel_b = '5X'\n\n# Filter DataFrames for each signal\n# Using a mask allows accessing filtered data directly\n# without creating new DataFrames\nmask_a = df['channel'] == channel_a\nmask_b = df['channel'] == channel_b\n\ndf_a = df[mask_a]\ndf_b = df[mask_b]\n\n# Wavelenghts will be required to convert from cycles to meters\nwavelength_a = 299792458/(154.0*10.23e6)\nwavelength_b = 299792458/(115.0*10.23e6)\n\n# Merge on 'time' and calculate the difference\ndf_merged = df_a.merge(df_b, on=['epoch', 'sat'], suffixes=('_a', '_b'))\ndf_merged['li_m'] = df_merged['phase_a'] * wavelength_a - df_merged['phase_b'] *wavelength_b\n\n\n\nCode\nsat = 'E04'\n\ndf_sat = df_merged[df_merged['sat'] == sat]\n\n# Time elapsed\nt0 = df_sat['epoch'].iloc[:1].values[0]\nx = (df_sat['epoch'] - t0).dt.total_seconds().to_numpy()\n\n# plot\nplt.xlabel(f'Time elapsed since {t0} [s]')\nplt.ylabel('LI [m]')\nplt.title(f'Geometry free combination of phases for {channel_a} and {channel_b} ({sat})')\nplt.plot(x, df_sat['li_m'], '.-')\n\n\n\n\n\nExample of geometry free combination (LI)\n\n\n\n\nThe primary limitation of this approach lies in the additional memory required to store the filtering masks. However, this overhead is significantly less substantial compared to creating entirely new DataFrames for each signal. The mask-based approach operates on references to the original DataFrame, avoiding the creation of new data structures.\nAs an alternative to the merge operation, the groupby function can be employed to group data by epoch and subsequently identify the specific channels within each group for combination."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#why-the-constellation-and-signal-fields-enhance-efficiency",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#why-the-constellation-and-signal-fields-enhance-efficiency",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Why the constellation and signal fields enhance efficiency?",
    "text": "Why the constellation and signal fields enhance efficiency?\nThe inclusion of constellation and signal fields within the DataFrame, albeit they may seem redudant, significantly accelerates data processing and analysis by streamlining operations and reducing computational overhead in the following way:\n\nOptimized groupby operations: The signal field directly identifies unique signal types, eliminating the need for complex groupby operations involving multiple fields like ['sat', 'channel']. This simplification leads to substantial performance gains, as groupby operations on single fields are considerably faster.\nEfficient constellation-level analysis: The constellation field provides direct access to constellation information, bypassing the need for time-consuming string manipulation and filtering of the sat field. This enables efficient constellation-level operations, such as counting satellites per constellation or calculating constellation-specific statistics.\n\nBy incorporating these dedicated fields, the DataFrame becomes more efficient and versatile, enabling swift and accurate analysis of GNSS data."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#a-note-on-file-size",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#a-note-on-file-size",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "A note on file size",
    "text": "A note on file size\nDespite the convenience of using parquet files for data processing and the substantial increase in the loading speed (plus the possibility of distributed processing when using Apache Spark instances), in terms of storage it stilss falls short competing with the de-facto standard Hatanaka + Gzip combo.\nTaking as an example the RINEX file ACSO00XXX_R_20241310000_01D_01S_MO.rnx (ACSO station, data for 1 day at 1 second interval, multi-constellation, multi-frequency), the different file sizes using various compression formats are shown in the table below:\n\n\n\ncompression\nsize\n\n\n\n\nUncompressed RINEX file (rnx)\n428 MB\n\n\nHatanaka + Gzip\n37 MB\n\n\nParquet\n141 MB\n\n\nParquet + Gzip\n107 MB\n\n\n\nIn the best case, the size of the compressed parquet is usually 3 times larger than the Hatanaka + Gzip combo, however there may be still some room for improvement if data within the file is organized to exploit parquet features such as run length encoding or RLE.\nSpecifically, RLE can significantly compress GNSS data containing repetitive sequences, such as timestamps that appear multiple times within the file. By reorganizing data within the Parquet file to exploit such features, it may be possible to achieve a more favorable compression ratio."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#conclusions",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#conclusions",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Conclusions",
    "text": "Conclusions\nBased on the analyzed data, several key observations emerge:\n\nEase of use:\n\nSimplified Preprocessing: Parquet files offer a streamlined approach to reading GNSS measurements compared to RINEX. The complex multi-step process involving gzip decompression, Hatanaka decompression, and parsing to binary is significantly simplified.\nDirect Integration: Parquet files can be directly downloaded and seamlessly integrated into data analysis frameworks like Pandas, eliminating the need for extensive preprocessing.\n\nData Volume: The combination of Hatanaka and Zip compression still offers a much smaller file size than parquet (on a ratio of 1 to 3), but using features such as run-length encoding (REL) may help reducing the parquet file size."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#use-of-ai",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#use-of-ai",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Use of AI",
    "text": "Use of AI\nArtificial Intelligence has been used to polish some text styling and correct some typos. Ideation and data processing has been done by the author."
  },
  {
    "objectID": "posts/parquet_format_for_gnss_measurements/index.html#footnotes",
    "href": "posts/parquet_format_for_gnss_measurements/index.html#footnotes",
    "title": "Using Apache parquet to store and process GNSS measurements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nContinuously Operating Receiving Station‚Ü©Ô∏é"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Catal√†English\n\n\nAqu√≠ trobar√†s contingut molt variat; desde sistemes de posicionament global (GNSS), desenvolupament de software, algoritmes i jocs de taula. Pot ser que sigui interessants o avorrides, dep√®n de tu.\nA m√©s, per complicar-ho una mica, hi ha una barreja de lleng√ºes bastant considerable (angl√®s, catal√†, espanyol i gallec), per√≤ suposo que sempre pots fer servir Google Translate si t‚Äôinteressa algun contingut que estigui en un idioma que no domines üòâ.\n\n\nHere you will find a medley of content; from navigation systems (GNSS), software development, algorithms and board games. They can be both interesting or boring, at the end this is up to you.\nIn addition, to make things a bit more complicated, there is a considerable mix of languages (English, Catalan, Spanish, and Galician), but I suppose you can always use Google Translate if you‚Äôre interested in any content that‚Äôs in a language you‚Äôre not fluent in üòâ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParticle filter demo: 2D rover position and velocity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGaussian Process Regression: A Beginner‚Äôs Primer\n\n\n\nTechnology\n\nEnglish\n\n\n\nA beginner introduction to Gaussian Process regression, covering its core ideas, practical intuition, and key limitations.\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nNative integration of Hatanaka for Python\n\n\n\nGNSS\n\nEnglish\n\n\n\nNative integration of Hatanaka compression in Python via the C-API increases processing speed. Other programming languages such as Rust can also take advantage of this refactoring.\n\n\n\n\n\nJan 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOn my journey as CTO at Rokubun\n\n\n\nProfessional\n\nEnglish\n\n\n\nSome takeaways that I learned during my journey as CTO at Rokubun, leading the technological strategy at the company.\n\n\n\n\n\nDec 18, 2024\n\n\nMiquel Garcia\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Apache parquet to store and process GNSS measurements\n\n\n\nGNSS\n\nEnglish\n\n\n\nLeverage Apache parquet to store and exchange GNSS measurement, usually stored in RINEX (text-based) format. This format can be quickly loaded in data structures such as pandas DataFrame for efficient data manipulation and processing.\n\n\n\n\n\nNov 25, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nOn the exploding size of GNSS measurement data files\n\n\n\nGNSS\n\nEnglish\n\n\n\nThe rapid growth of GNSS receiver networks, coupled with the proliferation of constellations and signals, has led to an exponential increase in data volume. This post uses the GNSS data repository of the CDDIS GNSS receiver network to illustrate this trend.\n\n\n\n\n\nOct 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTraduci√≥n do xogo ‚ÄúC√≥digo Secreto‚Äù √≥ galego\n\n\n\nJocs de taula\n\nGalego\n\n\n\nIdeas para a traduci√≥n das cartas de palabras do xogo ‚ÄúCodenames‚Äù\n\n\n\n\n\nOct 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCom fer el teu Cronocartes\n\n\n\nJocs de taula\n\nCatal√†\n\n\n\nApunts per crear la teva versi√≥ del joc de cartes hist√≤ric Cronocartes\n\n\n\n\n\nFeb 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGraph theory to model Ticket to ride\n\n\n\ngames\n\nEnglish\n\n\n\nHow to use Graph theory to build a model for the Ticket to ride boardgame as a first step towards customization and create your own maps.\n\n\n\n\n\nDec 24, 2023\n\n\n\n\n\nNo matching items"
  }
]